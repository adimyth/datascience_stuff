{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational AutoEncoder\n",
    "\n",
    "**Resources**\n",
    "* [Jeremy Jordan's Blog](https://www.jeremyjordan.me/variational-autoencoders/)\n",
    "* [Excellent Medium Article](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf)\n",
    "\n",
    "*Note*:- Content & resources are taken from the above links\n",
    "\n",
    "## AutoEncoders\n",
    "In general AutoEncoders, the Encoder converts input data into an *encoding vector where each dimension represents some learned attribute about the data*. It outputs a *single value* for each dimension.\n",
    "\n",
    "For a latent vector of dimension 6, autoencoder will learn descriptive attributes of faces such as skin color, whether or not the person is wearing glasses, etc. in an attempt to describe an observation in some compressed representation.\n",
    "\n",
    "![AutoEncoder](https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-16-at-10.24.11-PM.png)\n",
    "\n",
    "### Issues\n",
    "AutoEncoders are only good at reconstructing the original input data. It does not allow us to generate variations of data.  \n",
    "\n",
    "The fundamental problem with autoencoders, for generation, is that the latent space they convert their inputs to and where their encoded vectors lie, may not be continuous, or allow easy interpolation.\n",
    "\n",
    "Ideally, to generate new images we randomly sample from the latent space & pass them to decoder. If the space has discontinuities (eg. gaps between clusters) and you sample/generate a variation from there, the decoder will simply generate an unrealistic output or garbage\n",
    "\n",
    "![Distinct Clusters in latent space](https://miro.medium.com/max/1000/1*-i8cp3ry4XS-05OWPAJLPg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational AutoEncoders (VAEs)\n",
    "\n",
    "VAEs allow us to generate random, new output that looks similar to the training data.\n",
    "\n",
    "VAEs provide a probabilistic distribution for describing each attribute/feature in the latent space. Instead of describing the input using a single variable, they represent each latent variable as a range of possible values described using a probability distribution. VAEs have latent spaces that are continous, allowing easy random sampling and interpolation.\n",
    "\n",
    "![VAE](https://www.jeremyjordan.me/content/images/2018/06/Screen-Shot-2018-06-20-at-2.47.56-PM.png)\n",
    "\n",
    "When decoding, we randomly sample from each latent state distribution to generate vector as input for the decoder model.\n",
    "\n",
    "For any sampling of the latent distributions, we're expecting our decoder model to be able to accurately reconstruct the input i.e for nearby values for each featurer in the latent space distribution, shoule produce similar reconstructions.\n",
    "\n",
    "![Sampled Decoding](https://www.jeremyjordan.me/content/images/2018/06/Screen-Shot-2018-06-20-at-2.48.42-PM.png)\n",
    "\n",
    "### Solution\n",
    "Suppose that there exists some hidden variable $z$ which generates an observation $\\hat{x}$. To infer the characteristics of $z$ for observation $\\hat{x}$, compute - \n",
    "\n",
    "$$p\\left({z|\\hat{x}}\\right) = \\frac{{p\\left({\\hat{x}|z}\\right)p\\left( z \\right)}}{{p\\left(\\hat{x}\\right)}}$$\n",
    "\n",
    "Computing the denominator term is difficult. \n",
    "\n",
    "Variational Inference let's us approximate $p\\left({z|\\hat{x}}\\right)$ using another distribution $q\\left({z|\\hat{x}}\\right)$. We want to approximate $q\\left({z|\\hat{x}}\\right)$ similar to $p\\left({z|\\hat{x}}\\right)$, which can be done using KL Divergence -\n",
    "\n",
    "$$\\min KL\\left( {q\\left( {z|\\hat{x}} \\right)||p\\left( {z|\\hat{x}} \\right)} \\right)$$\n",
    "\n",
    "$${E_{q\\left( {z|\\hat{x}} \\right)}}\\log p\\left( {\\hat{x}|z} \\right) - KL\\left( {q\\left( {z|\\hat{x}} \\right)||p\\left( z \\right)} \\right)$$\n",
    "\n",
    "\n",
    "**My Understanding**\n",
    "\n",
    "Encoder essentailly gives us $q(z|x)$. We can use it to infer $q\\left({z|\\hat{x}}\\right)$, if $x$ and $\\hat{x}$ are very close to each other. It can be achieved using the reconstruction loss.\n",
    "\n",
    "Neural Network is encouraged to learn the distribution $q\\left({z|\\hat{x}}\\right)$ to be similar to the true distribution $p(z)$, which is assumed to be a unit Gaussian distribution. This could be achieved by adding the second term in the above equation to the loss function.\n",
    "\n",
    "$${\\cal L}\\left( {x,\\hat x} \\right) + \\sum\\limits_j {KL\\left( {{q_j}\\left( {z|x} \\right)||p\\left( z \\right)} \\right)}$$\n",
    "\n",
    "Second term \n",
    "\n",
    "\n",
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
