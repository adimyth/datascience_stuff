{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNormalization\n",
    "\n",
    "***Resources***\n",
    "[ML Explained](https://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/)\n",
    "\n",
    "## BatchNorm Limitations\n",
    "BatchNorm computes the mean and variance of each mini-batch and normalizes each feature according to the mini-batch statistics. \n",
    "\n",
    "Ideally, we want to calculate global mean and variance to normalize the inputs to a layer. However, computing mean of entire dataset for a single update is computationally expensive. Hence, we estimate them using mini-batch statistics, which might contain some error compared to global mean. Also, it varies from batch to batch. Hence, we have to be extra careful while choosing the batch size.\n",
    "\n",
    "BatchNorm is difficult to apply to RNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
