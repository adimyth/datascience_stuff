{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex NAC Layer\n",
    "\n",
    "$y=\\exp [W(\\log(|x|+\\varepsilon))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a series of input number X1 and X2 for training\n",
    "x1 = np.random.randint(low=0, high=10000, size=1000)\n",
    "x1 = x1.astype('float32')\n",
    "x2 = np.random.randint(low=100, high=10000, size=1000)\n",
    "x2 = x2.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "print(len(x1), len(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x1*x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.column_stack((x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a series of input number X1 and X2 for testing\n",
    "x1 = np.random.randint(low=0, high=10000, size=100)\n",
    "x1 = x1.astype('float32')\n",
    "x2 = np.random.randint(low=100, high=10000, size=100)\n",
    "x2 = x2.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.column_stack((x1,x2))\n",
    "y_test = x1*x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function**\n",
    "\n",
    "Using RMSE as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_loss(predicted_y, target_y):\n",
    "    predicted_y = tf.squeeze(predicted_y)\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log(predicted_y) - tf.math.log(target_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(predicted_y, target_y):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(predicted_y - target_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5000\n",
    "alpha = 0.1\n",
    "epsilon = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat = tf.Variable(shape=[2, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[2, 1], minval=-2, maxval=2),\n",
    "                        trainable=True, name=\"W_hat\")\n",
    "M_hat = tf.Variable(shape=[2, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[2, 1], minval=-2, maxval=2),\n",
    "                        trainable=True, name=\"M_hat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = tf.keras.optimizers.Adam(learning_rate=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Loss: 33640916.0\n",
      "RMSLE Loss: 14.38\n"
     ]
    }
   ],
   "source": [
    "W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "# Express input feature in log space to learn complex functions\n",
    "x_modified = tf.math.log(tf.abs(x_train) + epsilon)\n",
    "y_pred = tf.exp(tf.matmul(x_modified, W))\n",
    "print(f\"RMSE Loss: {rmse_loss(y_pred, y_train)}\")\n",
    "print(f\"RMSLE Loss: {rmsle_loss(y_pred, y_train):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, learning_rate=0.01):\n",
    "    with tf.GradientTape() as t:\n",
    "        # update weight matrix (W) -> restricted to [-1, 1]\n",
    "        W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "        # express input feature in log space to learn complex functions\n",
    "        x_modified = tf.math.log(tf.abs(X) + epsilon)\n",
    "        # make prediction\n",
    "        y_pred = tf.exp(tf.matmul(x_modified, W))\n",
    "        # calculate both loss\n",
    "        rmsle = rmsle_loss(y_pred, y)\n",
    "        rmse = rmse_loss(y_pred, y)\n",
    "    # calculate gradient w.r.t rmsle loss\n",
    "    dW_hat, dM_hat = t.gradient(rmsle, (W_hat, M_hat))\n",
    "    optimize.apply_gradients(zip([dW_hat, dM_hat], [W_hat, M_hat]))\n",
    "    return rmsle, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tRMSE Loss: 33640916.00\tRMSLE Loss: 14.38\n",
      "Epoch: 500\tRMSE Loss: 30705406.00\tRMSLE Loss: 0.02\n",
      "Epoch: 1000\tRMSE Loss: 30938534.00\tRMSLE Loss: 0.01\n",
      "Epoch: 1500\tRMSE Loss: 30999058.00\tRMSLE Loss: 0.00\n",
      "Epoch: 2000\tRMSE Loss: 31024222.00\tRMSLE Loss: 0.00\n",
      "Epoch: 2500\tRMSE Loss: 31037202.00\tRMSLE Loss: 0.00\n",
      "Epoch: 3000\tRMSE Loss: 31044764.00\tRMSLE Loss: 0.00\n",
      "Epoch: 3500\tRMSE Loss: 31049558.00\tRMSLE Loss: 0.00\n",
      "Epoch: 4000\tRMSE Loss: 31052750.00\tRMSLE Loss: 0.00\n",
      "Epoch: 4500\tRMSE Loss: 31054966.00\tRMSLE Loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    rmsle, rmse = train(x_train, y_train)\n",
    "    loss_history.append(rmsle)\n",
    "    if epoch%500==0:\n",
    "        print(f\"Epoch: {epoch}\\tRMSE Loss: {rmse:.2f}\\tRMSLE Loss: {rmsle:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13aa01a20>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(X):\n",
    "    W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "    x_modified = tf.math.log(tf.abs(X) + epsilon)\n",
    "    y_pred = tf.exp(tf.matmul(x_modified, W))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = get_pred(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [9732. 6506.]\tOutput: 63297476.00\tActual: 63316392.0\n",
      "Input: [6099. 8691.]\tOutput: 52990720.00\tActual: 53006408.0\n",
      "Input: [8555. 7007.]\tOutput: 59927016.00\tActual: 59944884.0\n",
      "Input: [2368. 8900.]\tOutput: 21069288.00\tActual: 21075200.0\n",
      "Input: [ 285. 1738.]\tOutput: 495221.56\tActual: 495330.0\n",
      "Input: [ 148. 5771.]\tOutput: 853913.50\tActual: 854108.0\n",
      "Input: [4037. 7028.]\tOutput: 28363942.00\tActual: 28372036.0\n",
      "Input: [6549. 1482.]\tOutput: 9703021.00\tActual: 9705618.0\n",
      "Input: [7121. 1503.]\tOutput: 10699985.00\tActual: 10702863.0\n",
      "Input: [9958. 9111.]\tOutput: 90699544.00\tActual: 90727336.0\n",
      "Input: [4895. 3392.]\tOutput: 16599255.00\tActual: 16603840.0\n",
      "Input: [5280. 9111.]\tOutput: 48091948.00\tActual: 48106080.0\n",
      "Input: [1012.  519.]\tOutput: 525112.94\tActual: 525228.0\n",
      "Input: [9715. 4617.]\tOutput: 44840992.00\tActual: 44854156.0\n",
      "Input: [2785. 3402.]\tOutput: 9472034.00\tActual: 9474570.0\n",
      "Input: [3725. 7957.]\tOutput: 29631360.00\tActual: 29639824.0\n",
      "Input: [4576. 9800.]\tOutput: 44831584.00\tActual: 44844800.0\n",
      "Input: [5782. 7970.]\tOutput: 46069000.00\tActual: 46082540.0\n",
      "Input: [6257. 5793.]\tOutput: 36236264.00\tActual: 36246800.0\n",
      "Input: [9793.  867.]\tOutput: 8488280.00\tActual: 8490531.0\n",
      "Input: [2408. 2144.]\tOutput: 5161425.00\tActual: 5162752.0\n",
      "Input: [ 340. 1649.]\tOutput: 560536.44\tActual: 560660.0\n",
      "Input: [2817. 6240.]\tOutput: 17573186.00\tActual: 17578080.0\n",
      "Input: [2496. 3816.]\tOutput: 9522192.00\tActual: 9524736.0\n",
      "Input: [5962. 4581.]\tOutput: 27304092.00\tActual: 27311922.0\n",
      "Input: [5299. 7998.]\tOutput: 42369012.00\tActual: 42381400.0\n",
      "Input: [9917. 6648.]\tOutput: 65908408.00\tActual: 65928216.0\n",
      "Input: [7574. 2393.]\tOutput: 18119540.00\tActual: 18124582.0\n",
      "Input: [1689. 4937.]\tOutput: 8336375.50\tActual: 8338593.0\n",
      "Input: [3267. 7858.]\tOutput: 25664796.00\tActual: 25672086.0\n",
      "Input: [4406. 6893.]\tOutput: 30361832.00\tActual: 30370558.0\n",
      "Input: [8453. 7738.]\tOutput: 65389640.00\tActual: 65409312.0\n",
      "Input: [1442. 8093.]\tOutput: 11666945.00\tActual: 11670106.0\n",
      "Input: [8597. 6107.]\tOutput: 52486260.00\tActual: 52501880.0\n",
      "Input: [4437. 5642.]\tOutput: 25026460.00\tActual: 25033554.0\n",
      "Input: [5435. 9192.]\tOutput: 49943752.00\tActual: 49958520.0\n",
      "Input: [8435. 6456.]\tOutput: 54440152.00\tActual: 54456360.0\n",
      "Input: [4107. 8533.]\tOutput: 35034884.00\tActual: 35045032.0\n",
      "Input: [ 560. 7480.]\tOutput: 4187733.25\tActual: 4188800.0\n",
      "Input: [8785. 2485.]\tOutput: 21824596.00\tActual: 21830724.0\n",
      "Input: [5548. 4368.]\tOutput: 24226794.00\tActual: 24233664.0\n",
      "Input: [1826.  771.]\tOutput: 1407513.50\tActual: 1407846.0\n",
      "Input: [7629.  263.]\tOutput: 2005943.88\tActual: 2006427.0\n",
      "Input: [8325. 8756.]\tOutput: 72871768.00\tActual: 72893696.0\n",
      "Input: [3138. 4692.]\tOutput: 14719455.00\tActual: 14723496.0\n",
      "Input: [7693. 4895.]\tOutput: 37646280.00\tActual: 37657236.0\n",
      "Input: [8264. 4898.]\tOutput: 40465208.00\tActual: 40477072.0\n",
      "Input: [7842. 6136.]\tOutput: 48104336.00\tActual: 48118512.0\n",
      "Input: [6721. 2455.]\tOutput: 16495481.00\tActual: 16500055.0\n",
      "Input: [7002. 5987.]\tOutput: 41908796.00\tActual: 41920976.0\n",
      "Input: [4225. 8061.]\tOutput: 34047900.00\tActual: 34057724.0\n",
      "Input: [2978. 5927.]\tOutput: 17645700.00\tActual: 17650606.0\n",
      "Input: [7373. 8837.]\tOutput: 65135580.00\tActual: 65155200.0\n",
      "Input: [5907. 4819.]\tOutput: 28457690.00\tActual: 28465832.0\n",
      "Input: [6097.  693.]\tOutput: 4224150.00\tActual: 4225221.0\n",
      "Input: [5656. 4939.]\tOutput: 27926970.00\tActual: 27934984.0\n",
      "Input: [8984. 7494.]\tOutput: 67305824.00\tActual: 67326096.0\n",
      "Input: [4259. 9751.]\tOutput: 41517360.00\tActual: 41529508.0\n",
      "Input: [8286. 3593.]\tOutput: 29763054.00\tActual: 29771598.0\n",
      "Input: [5901. 6264.]\tOutput: 36953216.00\tActual: 36963864.0\n",
      "Input: [9325. 2772.]\tOutput: 25841580.00\tActual: 25848900.0\n",
      "Input: [4917. 4300.]\tOutput: 21137152.00\tActual: 21143100.0\n",
      "Input: [8429.  116.]\tOutput: 977538.81\tActual: 977764.0\n",
      "Input: [2530. 3291.]\tOutput: 8324022.50\tActual: 8326230.0\n",
      "Input: [1960.  492.]\tOutput: 964099.44\tActual: 964320.0\n",
      "Input: [7045. 8716.]\tOutput: 61385836.00\tActual: 61404220.0\n",
      "Input: [1048. 6372.]\tOutput: 6676106.50\tActual: 6677856.0\n",
      "Input: [5772.  671.]\tOutput: 3872034.25\tActual: 3873012.0\n",
      "Input: [2529. 3731.]\tOutput: 9433181.00\tActual: 9435699.0\n",
      "Input: [2964. 6272.]\tOutput: 18585036.00\tActual: 18590208.0\n",
      "Input: [7137. 9412.]\tOutput: 67153232.00\tActual: 67173440.0\n",
      "Input: [ 963. 7277.]\tOutput: 7005907.00\tActual: 7007751.0\n",
      "Input: [ 536. 8533.]\tOutput: 4572520.50\tActual: 4573688.0\n",
      "Input: [7340. 3997.]\tOutput: 29329578.00\tActual: 29337980.0\n",
      "Input: [7179. 8223.]\tOutput: 59015268.00\tActual: 59032916.0\n",
      "Input: [7967. 2379.]\tOutput: 18948196.00\tActual: 18953492.0\n",
      "Input: [8734. 9080.]\tOutput: 79280840.00\tActual: 79304720.0\n",
      "Input: [ 845. 7551.]\tOutput: 6378933.00\tActual: 6380595.0\n",
      "Input: [5777. 9006.]\tOutput: 52012296.00\tActual: 52027664.0\n",
      "Input: [3036.  875.]\tOutput: 2655845.25\tActual: 2656500.0\n",
      "Input: [5947. 6969.]\tOutput: 41432556.00\tActual: 41444644.0\n",
      "Input: [2494. 2347.]\tOutput: 5851902.00\tActual: 5853418.0\n",
      "Input: [2719. 7188.]\tOutput: 19538718.00\tActual: 19544172.0\n",
      "Input: [1884.  178.]\tOutput: 335281.22\tActual: 335352.0\n",
      "Input: [4349. 8180.]\tOutput: 35564480.00\tActual: 35574820.0\n",
      "Input: [5179. 7199.]\tOutput: 37272820.00\tActual: 37283620.0\n",
      "Input: [6610. 4026.]\tOutput: 26604324.00\tActual: 26611860.0\n",
      "Input: [6430. 9147.]\tOutput: 58797748.00\tActual: 58815208.0\n",
      "Input: [7154. 7440.]\tOutput: 53209992.00\tActual: 53225760.0\n",
      "Input: [6488. 7183.]\tOutput: 46589648.00\tActual: 46603304.0\n",
      "Input: [4737. 6330.]\tOutput: 29976584.00\tActual: 29985210.0\n",
      "Input: [6525. 9323.]\tOutput: 60814332.00\tActual: 60832576.0\n",
      "Input: [3850. 5040.]\tOutput: 19398612.00\tActual: 19404000.0\n",
      "Input: [ 689. 3371.]\tOutput: 2322052.00\tActual: 2322619.0\n",
      "Input: [5175. 3556.]\tOutput: 18397160.00\tActual: 18402300.0\n",
      "Input: [8648. 7888.]\tOutput: 68194872.00\tActual: 68215424.0\n",
      "Input: [5078. 8699.]\tOutput: 44160536.00\tActual: 44173520.0\n",
      "Input: [5448. 7750.]\tOutput: 42209628.00\tActual: 42222000.0\n",
      "Input: [1179. 2811.]\tOutput: 3313341.50\tActual: 3314169.0\n",
      "Input: [9074. 1768.]\tOutput: 16038395.00\tActual: 16042832.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_test))[:100]:\n",
    "    print(f\"Input: {x_test[i]}\\tOutput: {y_test_pred[i].numpy()[0]:.2f}\\tActual: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = get_pred(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [7270. 9931.]\tOutput: 72176664.00\tActual: 72198368.0\tAbsolute Difference: 21704.00\n",
      "Input: [860. 927.]\tOutput: 797039.31\tActual: 797220.0\tAbsolute Difference: 180.69\n",
      "Input: [5390. 3631.]\tOutput: 19565606.00\tActual: 19571090.0\tAbsolute Difference: 5484.00\n",
      "Input: [5191. 7247.]\tOutput: 37608240.00\tActual: 37619176.0\tAbsolute Difference: 10936.00\n",
      "Input: [5734. 4551.]\tOutput: 26088058.00\tActual: 26095434.0\tAbsolute Difference: 7376.00\n",
      "Input: [6265. 3976.]\tOutput: 24902612.00\tActual: 24909640.0\tAbsolute Difference: 7028.00\n",
      "Input: [ 466. 6059.]\tOutput: 2822797.75\tActual: 2823494.0\tAbsolute Difference: 696.25\n",
      "Input: [4426.  226.]\tOutput: 1000046.00\tActual: 1000276.0\tAbsolute Difference: 230.00\n",
      "Input: [5578. 3277.]\tOutput: 18274024.00\tActual: 18279106.0\tAbsolute Difference: 5082.00\n",
      "Input: [8322. 8895.]\tOutput: 74001968.00\tActual: 74024192.0\tAbsolute Difference: 22224.00\n",
      "Input: [1685. 6124.]\tOutput: 10316152.00\tActual: 10318940.0\tAbsolute Difference: 2788.00\n",
      "Input: [ 769. 5209.]\tOutput: 4004707.00\tActual: 4005721.0\tAbsolute Difference: 1014.00\n",
      "Input: [6949. 4246.]\tOutput: 29497040.00\tActual: 29505454.0\tAbsolute Difference: 8414.00\n",
      "Input: [2433. 7296.]\tOutput: 17746216.00\tActual: 17751168.0\tAbsolute Difference: 4952.00\n",
      "Input: [5311. 3468.]\tOutput: 18413380.00\tActual: 18418548.0\tAbsolute Difference: 5168.00\n",
      "Input: [5051. 3897.]\tOutput: 19678256.00\tActual: 19683748.0\tAbsolute Difference: 5492.00\n",
      "Input: [6420. 1892.]\tOutput: 12143348.00\tActual: 12146640.0\tAbsolute Difference: 3292.00\n",
      "Input: [1184. 5832.]\tOutput: 6903272.50\tActual: 6905088.0\tAbsolute Difference: 1815.50\n",
      "Input: [4555. 3576.]\tOutput: 16284190.00\tActual: 16288680.0\tAbsolute Difference: 4490.00\n",
      "Input: [3385. 3517.]\tOutput: 11901822.00\tActual: 11905045.0\tAbsolute Difference: 3223.00\n",
      "Input: [6396. 1745.]\tOutput: 11157995.00\tActual: 11161020.0\tAbsolute Difference: 3025.00\n",
      "Input: [8666. 9497.]\tOutput: 82275976.00\tActual: 82301000.0\tAbsolute Difference: 25024.00\n",
      "Input: [9274. 3130.]\tOutput: 29019300.00\tActual: 29027620.0\tAbsolute Difference: 8320.00\n",
      "Input: [2558. 2972.]\tOutput: 7600367.00\tActual: 7602376.0\tAbsolute Difference: 2009.00\n",
      "Input: [7849. 3172.]\tOutput: 24889980.00\tActual: 24897028.0\tAbsolute Difference: 7048.00\n",
      "Input: [2047. 1954.]\tOutput: 3998826.00\tActual: 3999838.0\tAbsolute Difference: 1012.00\n",
      "Input: [2747. 4121.]\tOutput: 11317334.00\tActual: 11320387.0\tAbsolute Difference: 3053.00\n",
      "Input: [9167. 4762.]\tOutput: 43640472.00\tActual: 43653256.0\tAbsolute Difference: 12784.00\n",
      "Input: [9998. 6614.]\tOutput: 66106824.00\tActual: 66126772.0\tAbsolute Difference: 19948.00\n",
      "Input: [ 189. 5131.]\tOutput: 969535.75\tActual: 969759.0\tAbsolute Difference: 223.25\n",
      "Input: [2734. 2808.]\tOutput: 7675048.00\tActual: 7677072.0\tAbsolute Difference: 2024.00\n",
      "Input: [3005. 7212.]\tOutput: 21665992.00\tActual: 21672060.0\tAbsolute Difference: 6068.00\n",
      "Input: [4658. 9624.]\tOutput: 44815424.00\tActual: 44828592.0\tAbsolute Difference: 13168.00\n",
      "Input: [1899. 8493.]\tOutput: 16123755.00\tActual: 16128207.0\tAbsolute Difference: 4452.00\n",
      "Input: [7734. 3607.]\tOutput: 27888592.00\tActual: 27896538.0\tAbsolute Difference: 7946.00\n",
      "Input: [1267. 6556.]\tOutput: 8304247.50\tActual: 8306452.0\tAbsolute Difference: 2204.50\n",
      "Input: [1528. 4312.]\tOutput: 6587017.00\tActual: 6588736.0\tAbsolute Difference: 1719.00\n",
      "Input: [3556. 8070.]\tOutput: 28688710.00\tActual: 28696920.0\tAbsolute Difference: 8210.00\n",
      "Input: [3890. 8866.]\tOutput: 34478760.00\tActual: 34488740.0\tAbsolute Difference: 9980.00\n",
      "Input: [8838.  636.]\tOutput: 5619513.50\tActual: 5620968.0\tAbsolute Difference: 1454.50\n",
      "Input: [5393. 1725.]\tOutput: 9300440.00\tActual: 9302925.0\tAbsolute Difference: 2485.00\n",
      "Input: [8792. 8073.]\tOutput: 70956504.00\tActual: 70977816.0\tAbsolute Difference: 21312.00\n",
      "Input: [8433. 8677.]\tOutput: 73150976.00\tActual: 73173144.0\tAbsolute Difference: 22168.00\n",
      "Input: [7513. 8170.]\tOutput: 61362892.00\tActual: 61381208.0\tAbsolute Difference: 18316.00\n",
      "Input: [2612. 8766.]\tOutput: 22890310.00\tActual: 22896792.0\tAbsolute Difference: 6482.00\n",
      "Input: [7041. 8709.]\tOutput: 61301828.00\tActual: 61320068.0\tAbsolute Difference: 18240.00\n",
      "Input: [9555. 6590.]\tOutput: 62948564.00\tActual: 62967448.0\tAbsolute Difference: 18884.00\n",
      "Input: [6235. 3700.]\tOutput: 23062978.00\tActual: 23069500.0\tAbsolute Difference: 6522.00\n",
      "Input: [5486. 6883.]\tOutput: 37749172.00\tActual: 37760136.0\tAbsolute Difference: 10964.00\n",
      "Input: [7099. 9246.]\tOutput: 65617652.00\tActual: 65637352.0\tAbsolute Difference: 19700.00\n",
      "Input: [9670. 5071.]\tOutput: 49022148.00\tActual: 49036568.0\tAbsolute Difference: 14420.00\n",
      "Input: [ 775. 1305.]\tOutput: 1011142.56\tActual: 1011375.0\tAbsolute Difference: 232.44\n",
      "Input: [8226. 2090.]\tOutput: 17187562.00\tActual: 17192340.0\tAbsolute Difference: 4778.00\n",
      "Input: [3152. 5399.]\tOutput: 17012970.00\tActual: 17017648.0\tAbsolute Difference: 4678.00\n",
      "Input: [1585. 3672.]\tOutput: 5818601.50\tActual: 5820120.0\tAbsolute Difference: 1518.50\n",
      "Input: [3943. 6075.]\tOutput: 23946964.00\tActual: 23953724.0\tAbsolute Difference: 6760.00\n",
      "Input: [7555. 6420.]\tOutput: 48488832.00\tActual: 48503100.0\tAbsolute Difference: 14268.00\n",
      "Input: [3073. 4480.]\tOutput: 13763287.00\tActual: 13767040.0\tAbsolute Difference: 3753.00\n",
      "Input: [1021. 4857.]\tOutput: 4957718.50\tActual: 4958997.0\tAbsolute Difference: 1278.50\n",
      "Input: [3843. 2813.]\tOutput: 10807441.00\tActual: 10810359.0\tAbsolute Difference: 2918.00\n",
      "Input: [7989. 5688.]\tOutput: 45428100.00\tActual: 45441432.0\tAbsolute Difference: 13332.00\n",
      "Input: [9692. 1066.]\tOutput: 10328891.00\tActual: 10331672.0\tAbsolute Difference: 2781.00\n",
      "Input: [6873. 5044.]\tOutput: 34657372.00\tActual: 34667412.0\tAbsolute Difference: 10040.00\n",
      "Input: [5675. 3532.]\tOutput: 20038456.00\tActual: 20044100.0\tAbsolute Difference: 5644.00\n",
      "Input: [161. 788.]\tOutput: 126843.16\tActual: 126868.0\tAbsolute Difference: 24.84\n",
      "Input: [4297. 4343.]\tOutput: 18656674.00\tActual: 18661872.0\tAbsolute Difference: 5198.00\n",
      "Input: [ 995. 3801.]\tOutput: 3781040.25\tActual: 3781995.0\tAbsolute Difference: 954.75\n",
      "Input: [7629. 8633.]\tOutput: 65841436.00\tActual: 65861156.0\tAbsolute Difference: 19720.00\n",
      "Input: [9467. 5282.]\tOutput: 49989880.00\tActual: 50004696.0\tAbsolute Difference: 14816.00\n",
      "Input: [1016. 6804.]\tOutput: 6911052.00\tActual: 6912864.0\tAbsolute Difference: 1812.00\n",
      "Input: [7869. 4522.]\tOutput: 35573368.00\tActual: 35583616.0\tAbsolute Difference: 10248.00\n",
      "Input: [6439. 1859.]\tOutput: 11966855.00\tActual: 11970101.0\tAbsolute Difference: 3246.00\n",
      "Input: [7892. 9281.]\tOutput: 73223568.00\tActual: 73245648.0\tAbsolute Difference: 22080.00\n",
      "Input: [6863. 2582.]\tOutput: 17715338.00\tActual: 17720266.0\tAbsolute Difference: 4928.00\n",
      "Input: [7916. 8158.]\tOutput: 64559328.00\tActual: 64578728.0\tAbsolute Difference: 19400.00\n",
      "Input: [8529. 1051.]\tOutput: 8961581.00\tActual: 8963979.0\tAbsolute Difference: 2398.00\n",
      "Input: [ 878. 9014.]\tOutput: 7912203.50\tActual: 7914292.0\tAbsolute Difference: 2088.50\n",
      "Input: [9268. 2179.]\tOutput: 20189302.00\tActual: 20194972.0\tAbsolute Difference: 5670.00\n",
      "Input: [4887. 2817.]\tOutput: 13762920.00\tActual: 13766679.0\tAbsolute Difference: 3759.00\n",
      "Input: [4859. 4624.]\tOutput: 22461704.00\tActual: 22468016.0\tAbsolute Difference: 6312.00\n",
      "Input: [6331. 9792.]\tOutput: 61974528.00\tActual: 61993152.0\tAbsolute Difference: 18624.00\n",
      "Input: [8571. 8577.]\tOutput: 73491368.00\tActual: 73513464.0\tAbsolute Difference: 22096.00\n",
      "Input: [8684. 7411.]\tOutput: 64337940.00\tActual: 64357124.0\tAbsolute Difference: 19184.00\n",
      "Input: [7208. 7286.]\tOutput: 52501880.00\tActual: 52517488.0\tAbsolute Difference: 15608.00\n",
      "Input: [5276. 2165.]\tOutput: 11419462.00\tActual: 11422540.0\tAbsolute Difference: 3078.00\n",
      "Input: [2062.  868.]\tOutput: 1789385.38\tActual: 1789816.0\tAbsolute Difference: 430.62\n",
      "Input: [  64. 5590.]\tOutput: 357683.94\tActual: 357760.0\tAbsolute Difference: 76.06\n",
      "Input: [8006. 6745.]\tOutput: 53984464.00\tActual: 54000472.0\tAbsolute Difference: 16008.00\n",
      "Input: [2568. 5183.]\tOutput: 13306292.00\tActual: 13309944.0\tAbsolute Difference: 3652.00\n",
      "Input: [5463. 5332.]\tOutput: 29120378.00\tActual: 29128716.0\tAbsolute Difference: 8338.00\n",
      "Input: [2027. 5897.]\tOutput: 11949976.00\tActual: 11953219.0\tAbsolute Difference: 3243.00\n",
      "Input: [2695. 9238.]\tOutput: 24889364.00\tActual: 24896410.0\tAbsolute Difference: 7046.00\n",
      "Input: [9687. 5026.]\tOutput: 48672580.00\tActual: 48686864.0\tAbsolute Difference: 14284.00\n",
      "Input: [5258. 6116.]\tOutput: 32148650.00\tActual: 32157928.0\tAbsolute Difference: 9278.00\n",
      "Input: [5618. 3435.]\tOutput: 19292420.00\tActual: 19297830.0\tAbsolute Difference: 5410.00\n",
      "Input: [6736. 8138.]\tOutput: 54801344.00\tActual: 54817568.0\tAbsolute Difference: 16224.00\n",
      "Input: [ 391. 4973.]\tOutput: 1943975.38\tActual: 1944443.0\tAbsolute Difference: 467.62\n",
      "Input: [5892. 1581.]\tOutput: 9312759.00\tActual: 9315252.0\tAbsolute Difference: 2493.00\n",
      "Input: [3561. 6405.]\tOutput: 22801764.00\tActual: 22808204.0\tAbsolute Difference: 6440.00\n",
      "Input: [6184. 3551.]\tOutput: 21953226.00\tActual: 21959384.0\tAbsolute Difference: 6158.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f\"Input: {x_train[i]}\\tOutput: {y_train_pred[i].numpy()[0]:.2f}\\tActual: {y_train[i]}\\tAbsolute Difference: {tf.math.abs(y_train_pred[i]-y_train[i])[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [7270. 9931.]\tOutput: 72176664.00\tActual: 72198368.0\tAbsolute Difference: 21704.00\n",
      "Input: [860. 927.]\tOutput: 797039.31\tActual: 797220.0\tAbsolute Difference: 180.69\n",
      "Input: [5390. 3631.]\tOutput: 19565606.00\tActual: 19571090.0\tAbsolute Difference: 5484.00\n",
      "Input: [5191. 7247.]\tOutput: 37608240.00\tActual: 37619176.0\tAbsolute Difference: 10936.00\n",
      "Input: [5734. 4551.]\tOutput: 26088058.00\tActual: 26095434.0\tAbsolute Difference: 7376.00\n",
      "Input: [6265. 3976.]\tOutput: 24902612.00\tActual: 24909640.0\tAbsolute Difference: 7028.00\n",
      "Input: [ 466. 6059.]\tOutput: 2822797.75\tActual: 2823494.0\tAbsolute Difference: 696.25\n",
      "Input: [4426.  226.]\tOutput: 1000046.00\tActual: 1000276.0\tAbsolute Difference: 230.00\n",
      "Input: [5578. 3277.]\tOutput: 18274024.00\tActual: 18279106.0\tAbsolute Difference: 5082.00\n",
      "Input: [8322. 8895.]\tOutput: 74001968.00\tActual: 74024192.0\tAbsolute Difference: 22224.00\n",
      "Input: [1685. 6124.]\tOutput: 10316152.00\tActual: 10318940.0\tAbsolute Difference: 2788.00\n",
      "Input: [ 769. 5209.]\tOutput: 4004707.00\tActual: 4005721.0\tAbsolute Difference: 1014.00\n",
      "Input: [6949. 4246.]\tOutput: 29497040.00\tActual: 29505454.0\tAbsolute Difference: 8414.00\n",
      "Input: [2433. 7296.]\tOutput: 17746216.00\tActual: 17751168.0\tAbsolute Difference: 4952.00\n",
      "Input: [5311. 3468.]\tOutput: 18413380.00\tActual: 18418548.0\tAbsolute Difference: 5168.00\n",
      "Input: [5051. 3897.]\tOutput: 19678256.00\tActual: 19683748.0\tAbsolute Difference: 5492.00\n",
      "Input: [6420. 1892.]\tOutput: 12143348.00\tActual: 12146640.0\tAbsolute Difference: 3292.00\n",
      "Input: [1184. 5832.]\tOutput: 6903272.50\tActual: 6905088.0\tAbsolute Difference: 1815.50\n",
      "Input: [4555. 3576.]\tOutput: 16284190.00\tActual: 16288680.0\tAbsolute Difference: 4490.00\n",
      "Input: [3385. 3517.]\tOutput: 11901822.00\tActual: 11905045.0\tAbsolute Difference: 3223.00\n",
      "Input: [6396. 1745.]\tOutput: 11157995.00\tActual: 11161020.0\tAbsolute Difference: 3025.00\n",
      "Input: [8666. 9497.]\tOutput: 82275976.00\tActual: 82301000.0\tAbsolute Difference: 25024.00\n",
      "Input: [9274. 3130.]\tOutput: 29019300.00\tActual: 29027620.0\tAbsolute Difference: 8320.00\n",
      "Input: [2558. 2972.]\tOutput: 7600367.00\tActual: 7602376.0\tAbsolute Difference: 2009.00\n",
      "Input: [7849. 3172.]\tOutput: 24889980.00\tActual: 24897028.0\tAbsolute Difference: 7048.00\n",
      "Input: [2047. 1954.]\tOutput: 3998826.00\tActual: 3999838.0\tAbsolute Difference: 1012.00\n",
      "Input: [2747. 4121.]\tOutput: 11317334.00\tActual: 11320387.0\tAbsolute Difference: 3053.00\n",
      "Input: [9167. 4762.]\tOutput: 43640472.00\tActual: 43653256.0\tAbsolute Difference: 12784.00\n",
      "Input: [9998. 6614.]\tOutput: 66106824.00\tActual: 66126772.0\tAbsolute Difference: 19948.00\n",
      "Input: [ 189. 5131.]\tOutput: 969535.75\tActual: 969759.0\tAbsolute Difference: 223.25\n",
      "Input: [2734. 2808.]\tOutput: 7675048.00\tActual: 7677072.0\tAbsolute Difference: 2024.00\n",
      "Input: [3005. 7212.]\tOutput: 21665992.00\tActual: 21672060.0\tAbsolute Difference: 6068.00\n",
      "Input: [4658. 9624.]\tOutput: 44815424.00\tActual: 44828592.0\tAbsolute Difference: 13168.00\n",
      "Input: [1899. 8493.]\tOutput: 16123755.00\tActual: 16128207.0\tAbsolute Difference: 4452.00\n",
      "Input: [7734. 3607.]\tOutput: 27888592.00\tActual: 27896538.0\tAbsolute Difference: 7946.00\n",
      "Input: [1267. 6556.]\tOutput: 8304247.50\tActual: 8306452.0\tAbsolute Difference: 2204.50\n",
      "Input: [1528. 4312.]\tOutput: 6587017.00\tActual: 6588736.0\tAbsolute Difference: 1719.00\n",
      "Input: [3556. 8070.]\tOutput: 28688710.00\tActual: 28696920.0\tAbsolute Difference: 8210.00\n",
      "Input: [3890. 8866.]\tOutput: 34478760.00\tActual: 34488740.0\tAbsolute Difference: 9980.00\n",
      "Input: [8838.  636.]\tOutput: 5619513.50\tActual: 5620968.0\tAbsolute Difference: 1454.50\n",
      "Input: [5393. 1725.]\tOutput: 9300440.00\tActual: 9302925.0\tAbsolute Difference: 2485.00\n",
      "Input: [8792. 8073.]\tOutput: 70956504.00\tActual: 70977816.0\tAbsolute Difference: 21312.00\n",
      "Input: [8433. 8677.]\tOutput: 73150976.00\tActual: 73173144.0\tAbsolute Difference: 22168.00\n",
      "Input: [7513. 8170.]\tOutput: 61362892.00\tActual: 61381208.0\tAbsolute Difference: 18316.00\n",
      "Input: [2612. 8766.]\tOutput: 22890310.00\tActual: 22896792.0\tAbsolute Difference: 6482.00\n",
      "Input: [7041. 8709.]\tOutput: 61301828.00\tActual: 61320068.0\tAbsolute Difference: 18240.00\n",
      "Input: [9555. 6590.]\tOutput: 62948564.00\tActual: 62967448.0\tAbsolute Difference: 18884.00\n",
      "Input: [6235. 3700.]\tOutput: 23062978.00\tActual: 23069500.0\tAbsolute Difference: 6522.00\n",
      "Input: [5486. 6883.]\tOutput: 37749172.00\tActual: 37760136.0\tAbsolute Difference: 10964.00\n",
      "Input: [7099. 9246.]\tOutput: 65617652.00\tActual: 65637352.0\tAbsolute Difference: 19700.00\n",
      "Input: [9670. 5071.]\tOutput: 49022148.00\tActual: 49036568.0\tAbsolute Difference: 14420.00\n",
      "Input: [ 775. 1305.]\tOutput: 1011142.56\tActual: 1011375.0\tAbsolute Difference: 232.44\n",
      "Input: [8226. 2090.]\tOutput: 17187562.00\tActual: 17192340.0\tAbsolute Difference: 4778.00\n",
      "Input: [3152. 5399.]\tOutput: 17012970.00\tActual: 17017648.0\tAbsolute Difference: 4678.00\n",
      "Input: [1585. 3672.]\tOutput: 5818601.50\tActual: 5820120.0\tAbsolute Difference: 1518.50\n",
      "Input: [3943. 6075.]\tOutput: 23946964.00\tActual: 23953724.0\tAbsolute Difference: 6760.00\n",
      "Input: [7555. 6420.]\tOutput: 48488832.00\tActual: 48503100.0\tAbsolute Difference: 14268.00\n",
      "Input: [3073. 4480.]\tOutput: 13763287.00\tActual: 13767040.0\tAbsolute Difference: 3753.00\n",
      "Input: [1021. 4857.]\tOutput: 4957718.50\tActual: 4958997.0\tAbsolute Difference: 1278.50\n",
      "Input: [3843. 2813.]\tOutput: 10807441.00\tActual: 10810359.0\tAbsolute Difference: 2918.00\n",
      "Input: [7989. 5688.]\tOutput: 45428100.00\tActual: 45441432.0\tAbsolute Difference: 13332.00\n",
      "Input: [9692. 1066.]\tOutput: 10328891.00\tActual: 10331672.0\tAbsolute Difference: 2781.00\n",
      "Input: [6873. 5044.]\tOutput: 34657372.00\tActual: 34667412.0\tAbsolute Difference: 10040.00\n",
      "Input: [5675. 3532.]\tOutput: 20038456.00\tActual: 20044100.0\tAbsolute Difference: 5644.00\n",
      "Input: [161. 788.]\tOutput: 126843.16\tActual: 126868.0\tAbsolute Difference: 24.84\n",
      "Input: [4297. 4343.]\tOutput: 18656674.00\tActual: 18661872.0\tAbsolute Difference: 5198.00\n",
      "Input: [ 995. 3801.]\tOutput: 3781040.25\tActual: 3781995.0\tAbsolute Difference: 954.75\n",
      "Input: [7629. 8633.]\tOutput: 65841436.00\tActual: 65861156.0\tAbsolute Difference: 19720.00\n",
      "Input: [9467. 5282.]\tOutput: 49989880.00\tActual: 50004696.0\tAbsolute Difference: 14816.00\n",
      "Input: [1016. 6804.]\tOutput: 6911052.00\tActual: 6912864.0\tAbsolute Difference: 1812.00\n",
      "Input: [7869. 4522.]\tOutput: 35573368.00\tActual: 35583616.0\tAbsolute Difference: 10248.00\n",
      "Input: [6439. 1859.]\tOutput: 11966855.00\tActual: 11970101.0\tAbsolute Difference: 3246.00\n",
      "Input: [7892. 9281.]\tOutput: 73223568.00\tActual: 73245648.0\tAbsolute Difference: 22080.00\n",
      "Input: [6863. 2582.]\tOutput: 17715338.00\tActual: 17720266.0\tAbsolute Difference: 4928.00\n",
      "Input: [7916. 8158.]\tOutput: 64559328.00\tActual: 64578728.0\tAbsolute Difference: 19400.00\n",
      "Input: [8529. 1051.]\tOutput: 8961581.00\tActual: 8963979.0\tAbsolute Difference: 2398.00\n",
      "Input: [ 878. 9014.]\tOutput: 7912203.50\tActual: 7914292.0\tAbsolute Difference: 2088.50\n",
      "Input: [9268. 2179.]\tOutput: 20189302.00\tActual: 20194972.0\tAbsolute Difference: 5670.00\n",
      "Input: [4887. 2817.]\tOutput: 13762920.00\tActual: 13766679.0\tAbsolute Difference: 3759.00\n",
      "Input: [4859. 4624.]\tOutput: 22461704.00\tActual: 22468016.0\tAbsolute Difference: 6312.00\n",
      "Input: [6331. 9792.]\tOutput: 61974528.00\tActual: 61993152.0\tAbsolute Difference: 18624.00\n",
      "Input: [8571. 8577.]\tOutput: 73491368.00\tActual: 73513464.0\tAbsolute Difference: 22096.00\n",
      "Input: [8684. 7411.]\tOutput: 64337940.00\tActual: 64357124.0\tAbsolute Difference: 19184.00\n",
      "Input: [7208. 7286.]\tOutput: 52501880.00\tActual: 52517488.0\tAbsolute Difference: 15608.00\n",
      "Input: [5276. 2165.]\tOutput: 11419462.00\tActual: 11422540.0\tAbsolute Difference: 3078.00\n",
      "Input: [2062.  868.]\tOutput: 1789385.38\tActual: 1789816.0\tAbsolute Difference: 430.62\n",
      "Input: [  64. 5590.]\tOutput: 357683.94\tActual: 357760.0\tAbsolute Difference: 76.06\n",
      "Input: [8006. 6745.]\tOutput: 53984464.00\tActual: 54000472.0\tAbsolute Difference: 16008.00\n",
      "Input: [2568. 5183.]\tOutput: 13306292.00\tActual: 13309944.0\tAbsolute Difference: 3652.00\n",
      "Input: [5463. 5332.]\tOutput: 29120378.00\tActual: 29128716.0\tAbsolute Difference: 8338.00\n",
      "Input: [2027. 5897.]\tOutput: 11949976.00\tActual: 11953219.0\tAbsolute Difference: 3243.00\n",
      "Input: [2695. 9238.]\tOutput: 24889364.00\tActual: 24896410.0\tAbsolute Difference: 7046.00\n",
      "Input: [9687. 5026.]\tOutput: 48672580.00\tActual: 48686864.0\tAbsolute Difference: 14284.00\n",
      "Input: [5258. 6116.]\tOutput: 32148650.00\tActual: 32157928.0\tAbsolute Difference: 9278.00\n",
      "Input: [5618. 3435.]\tOutput: 19292420.00\tActual: 19297830.0\tAbsolute Difference: 5410.00\n",
      "Input: [6736. 8138.]\tOutput: 54801344.00\tActual: 54817568.0\tAbsolute Difference: 16224.00\n",
      "Input: [ 391. 4973.]\tOutput: 1943975.38\tActual: 1944443.0\tAbsolute Difference: 467.62\n",
      "Input: [5892. 1581.]\tOutput: 9312759.00\tActual: 9315252.0\tAbsolute Difference: 2493.00\n",
      "Input: [3561. 6405.]\tOutput: 22801764.00\tActual: 22808204.0\tAbsolute Difference: 6440.00\n",
      "Input: [6184. 3551.]\tOutput: 21953226.00\tActual: 21959384.0\tAbsolute Difference: 6158.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f\"Input: {x_train[i]}\\tOutput: {y_train_pred[i].numpy()[0]:.2f}\\tActual: {y_train[i]}\\tAbsolute Difference: {tf.math.abs(y_train_pred[i]-y_train[i])[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a series of input number X1 and X2 for training\n",
    "x1 = np.random.randint(low=0, high=10000, size=1000)\n",
    "x1 = x1.astype('float32')\n",
    "x2 = np.random.randint(low=100, high=10000, size=1000)\n",
    "x2 = x2.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x1/x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.column_stack((x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n",
      "(1000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAANYElEQVR4nO3cbYid5Z3H8e9vnVqrZY0PQ7CT7E4WQ4sUusrgpriUxXTBh9L4wrou3W2QLHnjbm3t0qZ9U/bFgkKptbAIwWxJQbpKKhja0kWihd0XDZ1oqdW0OFg1CVGnXbXdlq4N/e+Lc2Uds5nMiTOTk7nm+4Ew98N15r7m5s43J/ecc1JVSJL68gejnoAkaekZd0nqkHGXpA4Zd0nqkHGXpA6NjXoCAJdeemlNTk6OehqStKIcOHDg51U1frJ9Z0XcJycnmZ6eHvU0JGlFSfLCfPu8LSNJHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTLuktQh4y5JHTor3qG6GJM7vj2yYz9/140jO7YknYrP3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjpk3CWpQ8Zdkjo0VNyTfDrJ00l+nOQbSc5LsiHJ/iQzSR5Mcm4b+862PtP2Ty7nDyBJ+v8WjHuSCeCTwFRVvR84B7gVuBu4p6ouB14FtrWHbANebdvvaeMkSWfQsLdlxoB3JRkDzgeOAtcCe9r+3cBNbXlLW6ft35wkSzNdSdIwFox7VR0BvgS8yCDqrwMHgNeq6lgbdhiYaMsTwKH22GNt/CUnft8k25NMJ5menZ1d7M8hSZpjmNsyFzF4Nr4BeA9wAXDdYg9cVTuraqqqpsbHxxf77SRJcwxzW+bDwM+qaraqfgc8DFwDrGm3aQDWAUfa8hFgPUDbfyHwiyWdtSTplIaJ+4vApiTnt3vnm4FngMeBm9uYrcAjbXlvW6ftf6yqaummLElayDD33Pcz+MXoE8BT7TE7gc8BdyaZYXBPfVd7yC7gkrb9TmDHMsxbknQKYwsPgar6IvDFEzY/B1x9krG/BT62+KlJkt4u36EqSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUIeMuSR0y7pLUoaHinmRNkj1JfpLkYJIPJrk4yaNJnm1fL2pjk+SrSWaS/CjJVcv7I0iSTjTsM/d7ge9W1fuADwAHgR3AvqraCOxr6wDXAxvbn+3AfUs6Y0nSghaMe5ILgQ8BuwCq6o2qeg3YAuxuw3YDN7XlLcDXa+D7wJokly35zCVJ8xrmmfsGYBb4WpInk9yf5AJgbVUdbWNeAta25Qng0JzHH27bJElnyDBxHwOuAu6rqiuBX/PmLRgAqqqAOp0DJ9meZDrJ9Ozs7Ok8VJK0gGHifhg4XFX72/oeBrF/+fjtlvb1lbb/CLB+zuPXtW1vUVU7q2qqqqbGx8ff7vwlSSexYNyr6iXgUJL3tk2bgWeAvcDWtm0r8Ehb3gt8or1qZhPw+pzbN5KkM2BsyHH/ADyQ5FzgOeA2Bv8wPJRkG/ACcEsb+x3gBmAG+E0bK0k6g4aKe1X9EJg6ya7NJxlbwO2LnJckaRF8h6okdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdci4S1KHjLskdWjouCc5J8mTSb7V1jck2Z9kJsmDSc5t29/Z1mfa/snlmbokaT6n88z9DuDgnPW7gXuq6nLgVWBb274NeLVtv6eNkySdQUPFPck64Ebg/rYe4FpgTxuyG7ipLW9p67T9m9t4SdIZMuwz968AnwV+39YvAV6rqmNt/TAw0ZYngEMAbf/rbfxbJNmeZDrJ9Ozs7NucviTpZBaMe5KPAK9U1YGlPHBV7ayqqaqaGh8fX8pvLUmr3tgQY64BPprkBuA84A+Be4E1Scbas/N1wJE2/giwHjicZAy4EPjFks9ckjSvBZ+5V9Xnq2pdVU0CtwKPVdXHgceBm9uwrcAjbXlvW6ftf6yqaklnLUk6pcW8zv1zwJ1JZhjcU9/Vtu8CLmnb7wR2LG6KkqTTNcxtmf9TVd8DvteWnwOuPsmY3wIfW4K5SZLeJt+hKkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdMu6S1CHjLkkdWjDuSdYneTzJM0meTnJH235xkkeTPNu+XtS2J8lXk8wk+VGSq5b7h5AkvdUwz9yPAZ+pqiuATcDtSa4AdgD7qmojsK+tA1wPbGx/tgP3LfmsJUmntGDcq+poVT3Rln8FHAQmgC3A7jZsN3BTW94CfL0Gvg+sSXLZks9ckjSv07rnnmQSuBLYD6ytqqNt10vA2rY8ARya87DDbduJ32t7kukk07Ozs6c5bUnSqQwd9yTvBr4JfKqqfjl3X1UVUKdz4KraWVVTVTU1Pj5+Og+VJC1gqLgneQeDsD9QVQ+3zS8fv93Svr7Sth8B1s95+Lq2TZJ0hgzzapkAu4CDVfXlObv2Alvb8lbgkTnbP9FeNbMJeH3O7RtJ0hkwNsSYa4C/BZ5K8sO27QvAXcBDSbYBLwC3tH3fAW4AZoDfALct6YwlSQtaMO5V9Z9A5tm9+STjC7h9kfOSJC2C71CVpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA4Zd0nqkHGXpA6NjXoCK9nkjm+P5LjP33XjSI4raeXwmbskdci4S1KHjLskdWhZ4p7kuiQ/TTKTZMdyHEOSNL8l/4VqknOAfwH+EjgM/CDJ3qp6ZqmPtVqN6he54C9zpZViOV4tczUwU1XPAST5N2ALYNw7MMp/WFYb/yHVYixH3CeAQ3PWDwN/duKgJNuB7W31v5P89G0e71Lg52/zsb3z3MzvrD83uXukhz/rz88InU3n5o/n2zGy17lX1U5g52K/T5Lpqppagil1x3MzP8/NqXl+5rdSzs1y/EL1CLB+zvq6tk2SdIYsR9x/AGxMsiHJucCtwN5lOI4kaR5Lflumqo4l+Xvg34FzgH+tqqeX+jhzLPrWTsc8N/Pz3Jya52d+K+LcpKpGPQdJ0hLzHaqS1CHjLkkdWtFx92MO3pRkfZLHkzyT5Okkd7TtFyd5NMmz7etFo57rqCQ5J8mTSb7V1jck2d+unwfbCwBWnSRrkuxJ8pMkB5N80OtmIMmn29+nHyf5RpLzVsp1s2LjPudjDq4HrgD+OskVo53VSB0DPlNVVwCbgNvb+dgB7KuqjcC+tr5a3QEcnLN+N3BPVV0OvApsG8msRu9e4LtV9T7gAwzO0aq/bpJMAJ8Epqrq/QxeIHIrK+S6WbFxZ87HHFTVG8DxjzlYlarqaFU90ZZ/xeAv6ASDc7K7DdsN3DSaGY5WknXAjcD9bT3AtcCeNmRVnpskFwIfAnYBVNUbVfUaXjfHjQHvSjIGnA8cZYVcNys57if7mIOJEc3lrJJkErgS2A+sraqjbddLwNoRTWvUvgJ8Fvh9W78EeK2qjrX11Xr9bABmga+1W1b3J7kArxuq6gjwJeBFBlF/HTjACrluVnLcdRJJ3g18E/hUVf1y7r4avO511b32NclHgFeq6sCo53IWGgOuAu6rqiuBX3PCLZhVfN1cxOB/MBuA9wAXANeNdFKnYSXH3Y85OEGSdzAI+wNV9XDb/HKSy9r+y4BXRjW/EboG+GiS5xncvruWwX3mNe2/27B6r5/DwOGq2t/W9zCIvdcNfBj4WVXNVtXvgIcZXEsr4rpZyXH3Yw7maPeQdwEHq+rLc3btBba25a3AI2d6bqNWVZ+vqnVVNcngOnmsqj4OPA7c3Iat1nPzEnAoyXvbps0MPp571V83DG7HbEpyfvv7dfzcrIjrZkW/QzXJDQzupR7/mIN/HvGURibJnwP/ATzFm/eVv8DgvvtDwB8BLwC3VNV/jWSSZ4EkfwH8Y1V9JMmfMHgmfzHwJPA3VfU/o5zfKCT5Uwa/aD4XeA64jcETv1V/3ST5J+CvGLwa7Ung7xjcYz/rr5sVHXdJ0smt5NsykqR5GHdJ6pBxl6QOGXdJ6pBxl6QOGXdJ6pBxl6QO/S8WPEMeRsJCxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of y lies between 0 & 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a series of input number X1 and X2 for testing\n",
    "x1 = np.random.randint(low=0, high=10000, size=100)\n",
    "x1 = x1.astype('float32')\n",
    "x2 = np.random.randint(low=100, high=10000, size=100)\n",
    "x2 = x2.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.column_stack((x1,x2))\n",
    "y_test = x1/x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "alpha = 0.1\n",
    "epsilon = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat = tf.Variable(shape=[2, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[2, 1], minval=-2, maxval=2),\n",
    "                        trainable=True, name=\"W_hat\")\n",
    "M_hat = tf.Variable(shape=[2, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[2, 1], minval=-2, maxval=2),\n",
    "                        trainable=True, name=\"M_hat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = tf.keras.optimizers.Adam(learning_rate=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Loss: 11.852006912231445\n",
      "RMSLE Loss: 2.85\n"
     ]
    }
   ],
   "source": [
    "W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "# Express input feature in log space to learn complex functions\n",
    "x_modified = tf.math.log(tf.abs(x_train) + epsilon)\n",
    "y_pred = tf.exp(tf.matmul(x_modified, W))\n",
    "print(f\"RMSE Loss: {rmse_loss(y_pred, y_train)}\")\n",
    "print(f\"RMSLE Loss: {rmsle_loss(y_pred, y_train):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, learning_rate=0.01):\n",
    "    with tf.GradientTape() as t:\n",
    "        # update weight matrix (W) -> restricted to [-1, 1]\n",
    "        W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "        # express input feature in log space to learn complex functions\n",
    "        x_modified = tf.math.log(tf.abs(X) + epsilon)\n",
    "        # make prediction\n",
    "        y_pred = tf.exp(tf.matmul(x_modified, W))\n",
    "        # calculate both loss\n",
    "        rmsle = rmsle_loss(y_pred, y)\n",
    "        rmse = rmse_loss(y_pred, y)\n",
    "    # calculate gradient w.r.t rmsle loss\n",
    "    dW_hat, dM_hat = t.gradient(rmsle, (W_hat, M_hat))\n",
    "    optimize.apply_gradients(zip([dW_hat, dM_hat], [W_hat, M_hat]))\n",
    "    return rmsle, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tRMSE Loss: 11.85\tRMSLE Loss: 2.8488261699676514\n",
      "Epoch: 500\tRMSE Loss: 8.57\tRMSLE Loss: 0.005965225864201784\n",
      "Epoch: 1000\tRMSE Loss: 8.62\tRMSLE Loss: 0.0018308021826669574\n",
      "Epoch: 1500\tRMSE Loss: 8.63\tRMSLE Loss: 0.0009020435390993953\n",
      "Epoch: 2000\tRMSE Loss: 8.63\tRMSLE Loss: 0.0005307287792675197\n",
      "Epoch: 2500\tRMSE Loss: 8.63\tRMSLE Loss: 0.0003425833419896662\n",
      "Epoch: 3000\tRMSE Loss: 8.64\tRMSLE Loss: 0.0002339521743124351\n",
      "Epoch: 3500\tRMSE Loss: 8.64\tRMSLE Loss: 0.00016538774070795625\n",
      "Epoch: 4000\tRMSE Loss: 8.64\tRMSLE Loss: 0.00012001246068393812\n",
      "Epoch: 4500\tRMSE Loss: 8.64\tRMSLE Loss: 8.859192894306034e-05\n",
      "Epoch: 5000\tRMSE Loss: 8.64\tRMSLE Loss: 6.637765909545124e-05\n",
      "Epoch: 5500\tRMSE Loss: 8.64\tRMSLE Loss: 5.018784577259794e-05\n",
      "Epoch: 6000\tRMSE Loss: 8.64\tRMSLE Loss: 3.81501522497274e-05\n",
      "Epoch: 6500\tRMSE Loss: 8.64\tRMSLE Loss: 2.9216074835858308e-05\n",
      "Epoch: 7000\tRMSE Loss: 8.64\tRMSLE Loss: 2.2349351638695225e-05\n",
      "Epoch: 7500\tRMSE Loss: 8.64\tRMSLE Loss: 1.7331854905933142e-05\n",
      "Epoch: 8000\tRMSE Loss: 8.64\tRMSLE Loss: 1.3466830750985537e-05\n",
      "Epoch: 8500\tRMSE Loss: 8.64\tRMSLE Loss: 1.0421269507787656e-05\n",
      "Epoch: 9000\tRMSE Loss: 8.64\tRMSLE Loss: 8.005369636521209e-06\n",
      "Epoch: 9500\tRMSE Loss: 8.64\tRMSLE Loss: 5.952338597126072e-06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    rmsle, rmse = train(x_train, y_train)\n",
    "    loss_history.append(rmsle)\n",
    "    if epoch%500==0:\n",
    "        print(f\"Epoch: {epoch}\\tRMSE Loss: {rmse:.2f}\\tRMSLE Loss: {rmsle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13ae719b0>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(X):\n",
    "    W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "    x_modified = tf.math.log(tf.abs(X) + epsilon)\n",
    "    y_pred = tf.exp(tf.matmul(x_modified, W))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = get_pred(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [9732. 6506.]\tOutput: 1.50\tActual: 1.50\n",
      "Input: [6099. 8691.]\tOutput: 0.70\tActual: 0.70\n",
      "Input: [8555. 7007.]\tOutput: 1.22\tActual: 1.22\n",
      "Input: [2368. 8900.]\tOutput: 0.27\tActual: 0.27\n",
      "Input: [ 285. 1738.]\tOutput: 0.16\tActual: 0.16\n",
      "Input: [ 148. 5771.]\tOutput: 0.03\tActual: 0.03\n",
      "Input: [4037. 7028.]\tOutput: 0.57\tActual: 0.57\n",
      "Input: [6549. 1482.]\tOutput: 4.42\tActual: 4.42\n",
      "Input: [7121. 1503.]\tOutput: 4.74\tActual: 4.74\n",
      "Input: [9958. 9111.]\tOutput: 1.09\tActual: 1.09\n",
      "Input: [4895. 3392.]\tOutput: 1.44\tActual: 1.44\n",
      "Input: [5280. 9111.]\tOutput: 0.58\tActual: 0.58\n",
      "Input: [1012.  519.]\tOutput: 1.95\tActual: 1.95\n",
      "Input: [9715. 4617.]\tOutput: 2.10\tActual: 2.10\n",
      "Input: [2785. 3402.]\tOutput: 0.82\tActual: 0.82\n",
      "Input: [3725. 7957.]\tOutput: 0.47\tActual: 0.47\n",
      "Input: [4576. 9800.]\tOutput: 0.47\tActual: 0.47\n",
      "Input: [5782. 7970.]\tOutput: 0.73\tActual: 0.73\n",
      "Input: [6257. 5793.]\tOutput: 1.08\tActual: 1.08\n",
      "Input: [9793.  867.]\tOutput: 11.30\tActual: 11.30\n",
      "Input: [2408. 2144.]\tOutput: 1.12\tActual: 1.12\n",
      "Input: [ 340. 1649.]\tOutput: 0.21\tActual: 0.21\n",
      "Input: [2817. 6240.]\tOutput: 0.45\tActual: 0.45\n",
      "Input: [2496. 3816.]\tOutput: 0.65\tActual: 0.65\n",
      "Input: [5962. 4581.]\tOutput: 1.30\tActual: 1.30\n",
      "Input: [5299. 7998.]\tOutput: 0.66\tActual: 0.66\n",
      "Input: [9917. 6648.]\tOutput: 1.49\tActual: 1.49\n",
      "Input: [7574. 2393.]\tOutput: 3.17\tActual: 3.17\n",
      "Input: [1689. 4937.]\tOutput: 0.34\tActual: 0.34\n",
      "Input: [3267. 7858.]\tOutput: 0.42\tActual: 0.42\n",
      "Input: [4406. 6893.]\tOutput: 0.64\tActual: 0.64\n",
      "Input: [8453. 7738.]\tOutput: 1.09\tActual: 1.09\n",
      "Input: [1442. 8093.]\tOutput: 0.18\tActual: 0.18\n",
      "Input: [8597. 6107.]\tOutput: 1.41\tActual: 1.41\n",
      "Input: [4437. 5642.]\tOutput: 0.79\tActual: 0.79\n",
      "Input: [5435. 9192.]\tOutput: 0.59\tActual: 0.59\n",
      "Input: [8435. 6456.]\tOutput: 1.31\tActual: 1.31\n",
      "Input: [4107. 8533.]\tOutput: 0.48\tActual: 0.48\n",
      "Input: [ 560. 7480.]\tOutput: 0.07\tActual: 0.07\n",
      "Input: [8785. 2485.]\tOutput: 3.54\tActual: 3.54\n",
      "Input: [5548. 4368.]\tOutput: 1.27\tActual: 1.27\n",
      "Input: [1826.  771.]\tOutput: 2.37\tActual: 2.37\n",
      "Input: [7629.  263.]\tOutput: 29.01\tActual: 29.01\n",
      "Input: [8325. 8756.]\tOutput: 0.95\tActual: 0.95\n",
      "Input: [3138. 4692.]\tOutput: 0.67\tActual: 0.67\n",
      "Input: [7693. 4895.]\tOutput: 1.57\tActual: 1.57\n",
      "Input: [8264. 4898.]\tOutput: 1.69\tActual: 1.69\n",
      "Input: [7842. 6136.]\tOutput: 1.28\tActual: 1.28\n",
      "Input: [6721. 2455.]\tOutput: 2.74\tActual: 2.74\n",
      "Input: [7002. 5987.]\tOutput: 1.17\tActual: 1.17\n",
      "Input: [4225. 8061.]\tOutput: 0.52\tActual: 0.52\n",
      "Input: [2978. 5927.]\tOutput: 0.50\tActual: 0.50\n",
      "Input: [7373. 8837.]\tOutput: 0.83\tActual: 0.83\n",
      "Input: [5907. 4819.]\tOutput: 1.23\tActual: 1.23\n",
      "Input: [6097.  693.]\tOutput: 8.80\tActual: 8.80\n",
      "Input: [5656. 4939.]\tOutput: 1.15\tActual: 1.15\n",
      "Input: [8984. 7494.]\tOutput: 1.20\tActual: 1.20\n",
      "Input: [4259. 9751.]\tOutput: 0.44\tActual: 0.44\n",
      "Input: [8286. 3593.]\tOutput: 2.31\tActual: 2.31\n",
      "Input: [5901. 6264.]\tOutput: 0.94\tActual: 0.94\n",
      "Input: [9325. 2772.]\tOutput: 3.36\tActual: 3.36\n",
      "Input: [4917. 4300.]\tOutput: 1.14\tActual: 1.14\n",
      "Input: [8429.  116.]\tOutput: 72.66\tActual: 72.66\n",
      "Input: [2530. 3291.]\tOutput: 0.77\tActual: 0.77\n",
      "Input: [1960.  492.]\tOutput: 3.98\tActual: 3.98\n",
      "Input: [7045. 8716.]\tOutput: 0.81\tActual: 0.81\n",
      "Input: [1048. 6372.]\tOutput: 0.16\tActual: 0.16\n",
      "Input: [5772.  671.]\tOutput: 8.60\tActual: 8.60\n",
      "Input: [2529. 3731.]\tOutput: 0.68\tActual: 0.68\n",
      "Input: [2964. 6272.]\tOutput: 0.47\tActual: 0.47\n",
      "Input: [7137. 9412.]\tOutput: 0.76\tActual: 0.76\n",
      "Input: [ 963. 7277.]\tOutput: 0.13\tActual: 0.13\n",
      "Input: [ 536. 8533.]\tOutput: 0.06\tActual: 0.06\n",
      "Input: [7340. 3997.]\tOutput: 1.84\tActual: 1.84\n",
      "Input: [7179. 8223.]\tOutput: 0.87\tActual: 0.87\n",
      "Input: [7967. 2379.]\tOutput: 3.35\tActual: 3.35\n",
      "Input: [8734. 9080.]\tOutput: 0.96\tActual: 0.96\n",
      "Input: [ 845. 7551.]\tOutput: 0.11\tActual: 0.11\n",
      "Input: [5777. 9006.]\tOutput: 0.64\tActual: 0.64\n",
      "Input: [3036.  875.]\tOutput: 3.47\tActual: 3.47\n",
      "Input: [5947. 6969.]\tOutput: 0.85\tActual: 0.85\n",
      "Input: [2494. 2347.]\tOutput: 1.06\tActual: 1.06\n",
      "Input: [2719. 7188.]\tOutput: 0.38\tActual: 0.38\n",
      "Input: [1884.  178.]\tOutput: 10.58\tActual: 10.58\n",
      "Input: [4349. 8180.]\tOutput: 0.53\tActual: 0.53\n",
      "Input: [5179. 7199.]\tOutput: 0.72\tActual: 0.72\n",
      "Input: [6610. 4026.]\tOutput: 1.64\tActual: 1.64\n",
      "Input: [6430. 9147.]\tOutput: 0.70\tActual: 0.70\n",
      "Input: [7154. 7440.]\tOutput: 0.96\tActual: 0.96\n",
      "Input: [6488. 7183.]\tOutput: 0.90\tActual: 0.90\n",
      "Input: [4737. 6330.]\tOutput: 0.75\tActual: 0.75\n",
      "Input: [6525. 9323.]\tOutput: 0.70\tActual: 0.70\n",
      "Input: [3850. 5040.]\tOutput: 0.76\tActual: 0.76\n",
      "Input: [ 689. 3371.]\tOutput: 0.20\tActual: 0.20\n",
      "Input: [5175. 3556.]\tOutput: 1.46\tActual: 1.46\n",
      "Input: [8648. 7888.]\tOutput: 1.10\tActual: 1.10\n",
      "Input: [5078. 8699.]\tOutput: 0.58\tActual: 0.58\n",
      "Input: [5448. 7750.]\tOutput: 0.70\tActual: 0.70\n",
      "Input: [1179. 2811.]\tOutput: 0.42\tActual: 0.42\n",
      "Input: [9074. 1768.]\tOutput: 5.13\tActual: 5.13\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_test))[:100]:\n",
    "    print(f\"Input: {x_test[i]}\\tOutput: {y_test_pred[i].numpy()[0]:.2f}\\tActual: {y_test[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = get_pred(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [7270. 9931.]\tOutput: 0.73\tActual: 0.73\n",
      "Input: [860. 927.]\tOutput: 0.93\tActual: 0.93\n",
      "Input: [5390. 3631.]\tOutput: 1.48\tActual: 1.48\n",
      "Input: [5191. 7247.]\tOutput: 0.72\tActual: 0.72\n",
      "Input: [5734. 4551.]\tOutput: 1.26\tActual: 1.26\n",
      "Input: [6265. 3976.]\tOutput: 1.58\tActual: 1.58\n",
      "Input: [ 466. 6059.]\tOutput: 0.08\tActual: 0.08\n",
      "Input: [4426.  226.]\tOutput: 19.58\tActual: 19.58\n",
      "Input: [5578. 3277.]\tOutput: 1.70\tActual: 1.70\n",
      "Input: [8322. 8895.]\tOutput: 0.94\tActual: 0.94\n",
      "Input: [1685. 6124.]\tOutput: 0.28\tActual: 0.28\n",
      "Input: [ 769. 5209.]\tOutput: 0.15\tActual: 0.15\n",
      "Input: [6949. 4246.]\tOutput: 1.64\tActual: 1.64\n",
      "Input: [2433. 7296.]\tOutput: 0.33\tActual: 0.33\n",
      "Input: [5311. 3468.]\tOutput: 1.53\tActual: 1.53\n",
      "Input: [5051. 3897.]\tOutput: 1.30\tActual: 1.30\n",
      "Input: [6420. 1892.]\tOutput: 3.39\tActual: 3.39\n",
      "Input: [1184. 5832.]\tOutput: 0.20\tActual: 0.20\n",
      "Input: [4555. 3576.]\tOutput: 1.27\tActual: 1.27\n",
      "Input: [3385. 3517.]\tOutput: 0.96\tActual: 0.96\n",
      "Input: [6396. 1745.]\tOutput: 3.67\tActual: 3.67\n",
      "Input: [8666. 9497.]\tOutput: 0.91\tActual: 0.91\n",
      "Input: [9274. 3130.]\tOutput: 2.96\tActual: 2.96\n",
      "Input: [2558. 2972.]\tOutput: 0.86\tActual: 0.86\n",
      "Input: [7849. 3172.]\tOutput: 2.47\tActual: 2.47\n",
      "Input: [2047. 1954.]\tOutput: 1.05\tActual: 1.05\n",
      "Input: [2747. 4121.]\tOutput: 0.67\tActual: 0.67\n",
      "Input: [9167. 4762.]\tOutput: 1.93\tActual: 1.93\n",
      "Input: [9998. 6614.]\tOutput: 1.51\tActual: 1.51\n",
      "Input: [ 189. 5131.]\tOutput: 0.04\tActual: 0.04\n",
      "Input: [2734. 2808.]\tOutput: 0.97\tActual: 0.97\n",
      "Input: [3005. 7212.]\tOutput: 0.42\tActual: 0.42\n",
      "Input: [4658. 9624.]\tOutput: 0.48\tActual: 0.48\n",
      "Input: [1899. 8493.]\tOutput: 0.22\tActual: 0.22\n",
      "Input: [7734. 3607.]\tOutput: 2.14\tActual: 2.14\n",
      "Input: [1267. 6556.]\tOutput: 0.19\tActual: 0.19\n",
      "Input: [1528. 4312.]\tOutput: 0.35\tActual: 0.35\n",
      "Input: [3556. 8070.]\tOutput: 0.44\tActual: 0.44\n",
      "Input: [3890. 8866.]\tOutput: 0.44\tActual: 0.44\n",
      "Input: [8838.  636.]\tOutput: 13.90\tActual: 13.90\n",
      "Input: [5393. 1725.]\tOutput: 3.13\tActual: 3.13\n",
      "Input: [8792. 8073.]\tOutput: 1.09\tActual: 1.09\n",
      "Input: [8433. 8677.]\tOutput: 0.97\tActual: 0.97\n",
      "Input: [7513. 8170.]\tOutput: 0.92\tActual: 0.92\n",
      "Input: [2612. 8766.]\tOutput: 0.30\tActual: 0.30\n",
      "Input: [7041. 8709.]\tOutput: 0.81\tActual: 0.81\n",
      "Input: [9555. 6590.]\tOutput: 1.45\tActual: 1.45\n",
      "Input: [6235. 3700.]\tOutput: 1.69\tActual: 1.69\n",
      "Input: [5486. 6883.]\tOutput: 0.80\tActual: 0.80\n",
      "Input: [7099. 9246.]\tOutput: 0.77\tActual: 0.77\n",
      "Input: [9670. 5071.]\tOutput: 1.91\tActual: 1.91\n",
      "Input: [ 775. 1305.]\tOutput: 0.59\tActual: 0.59\n",
      "Input: [8226. 2090.]\tOutput: 3.94\tActual: 3.94\n",
      "Input: [3152. 5399.]\tOutput: 0.58\tActual: 0.58\n",
      "Input: [1585. 3672.]\tOutput: 0.43\tActual: 0.43\n",
      "Input: [3943. 6075.]\tOutput: 0.65\tActual: 0.65\n",
      "Input: [7555. 6420.]\tOutput: 1.18\tActual: 1.18\n",
      "Input: [3073. 4480.]\tOutput: 0.69\tActual: 0.69\n",
      "Input: [1021. 4857.]\tOutput: 0.21\tActual: 0.21\n",
      "Input: [3843. 2813.]\tOutput: 1.37\tActual: 1.37\n",
      "Input: [7989. 5688.]\tOutput: 1.40\tActual: 1.40\n",
      "Input: [9692. 1066.]\tOutput: 9.09\tActual: 9.09\n",
      "Input: [6873. 5044.]\tOutput: 1.36\tActual: 1.36\n",
      "Input: [5675. 3532.]\tOutput: 1.61\tActual: 1.61\n",
      "Input: [161. 788.]\tOutput: 0.20\tActual: 0.20\n",
      "Input: [4297. 4343.]\tOutput: 0.99\tActual: 0.99\n",
      "Input: [ 995. 3801.]\tOutput: 0.26\tActual: 0.26\n",
      "Input: [7629. 8633.]\tOutput: 0.88\tActual: 0.88\n",
      "Input: [9467. 5282.]\tOutput: 1.79\tActual: 1.79\n",
      "Input: [1016. 6804.]\tOutput: 0.15\tActual: 0.15\n",
      "Input: [7869. 4522.]\tOutput: 1.74\tActual: 1.74\n",
      "Input: [6439. 1859.]\tOutput: 3.46\tActual: 3.46\n",
      "Input: [7892. 9281.]\tOutput: 0.85\tActual: 0.85\n",
      "Input: [6863. 2582.]\tOutput: 2.66\tActual: 2.66\n",
      "Input: [7916. 8158.]\tOutput: 0.97\tActual: 0.97\n",
      "Input: [8529. 1051.]\tOutput: 8.12\tActual: 8.12\n",
      "Input: [ 878. 9014.]\tOutput: 0.10\tActual: 0.10\n",
      "Input: [9268. 2179.]\tOutput: 4.25\tActual: 4.25\n",
      "Input: [4887. 2817.]\tOutput: 1.73\tActual: 1.73\n",
      "Input: [4859. 4624.]\tOutput: 1.05\tActual: 1.05\n",
      "Input: [6331. 9792.]\tOutput: 0.65\tActual: 0.65\n",
      "Input: [8571. 8577.]\tOutput: 1.00\tActual: 1.00\n",
      "Input: [8684. 7411.]\tOutput: 1.17\tActual: 1.17\n",
      "Input: [7208. 7286.]\tOutput: 0.99\tActual: 0.99\n",
      "Input: [5276. 2165.]\tOutput: 2.44\tActual: 2.44\n",
      "Input: [2062.  868.]\tOutput: 2.38\tActual: 2.38\n",
      "Input: [  64. 5590.]\tOutput: 0.01\tActual: 0.01\n",
      "Input: [8006. 6745.]\tOutput: 1.19\tActual: 1.19\n",
      "Input: [2568. 5183.]\tOutput: 0.50\tActual: 0.50\n",
      "Input: [5463. 5332.]\tOutput: 1.02\tActual: 1.02\n",
      "Input: [2027. 5897.]\tOutput: 0.34\tActual: 0.34\n",
      "Input: [2695. 9238.]\tOutput: 0.29\tActual: 0.29\n",
      "Input: [9687. 5026.]\tOutput: 1.93\tActual: 1.93\n",
      "Input: [5258. 6116.]\tOutput: 0.86\tActual: 0.86\n",
      "Input: [5618. 3435.]\tOutput: 1.64\tActual: 1.64\n",
      "Input: [6736. 8138.]\tOutput: 0.83\tActual: 0.83\n",
      "Input: [ 391. 4973.]\tOutput: 0.08\tActual: 0.08\n",
      "Input: [5892. 1581.]\tOutput: 3.73\tActual: 3.73\n",
      "Input: [3561. 6405.]\tOutput: 0.56\tActual: 0.56\n",
      "Input: [6184. 3551.]\tOutput: 1.74\tActual: 1.74\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(f\"Input: {x_train[i]}\\tOutput: {y_train_pred[i].numpy()[0]:.2f}\\tActual: {y_train[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out of range\n",
    "\n",
    "In the training set both *x1 & x2* were random numbers within **0-10000**. Let's again create another test set with *x1* between **50000-100000** & *x2* from **-100 to 0** i.e negative numbers. This will definitely be different then the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerator will be between 50000-100000\n",
    "x1 = np.random.randint(low=50000, high=100000, size=100)\n",
    "x1 = x1.astype('float32')\n",
    "# denominator will be between -100 to 0\n",
    "x2 = np.random.randint(low=-100, high=0, size=100)\n",
    "x2 = x2.astype('float32')\n",
    "\n",
    "x_test = np.column_stack((x1,x2))\n",
    "y_test = x1/x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -30912.0\tMax: -606.550537109375\n"
     ]
    }
   ],
   "source": [
    "print(f\"Min: {min(y_test)}\\tMax: {max(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = get_pred(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [ 6.6531e+04 -5.1000e+01]\tOutput: 1304.50\tActual: -1304.53\n",
      "Input: [ 7.2804e+04 -6.0000e+00]\tOutput: 12133.59\tActual: -12134.00\n",
      "Input: [ 7.8903e+04 -2.2000e+01]\tOutput: 3586.40\tActual: -3586.50\n",
      "Input: [ 9.4857e+04 -7.8000e+01]\tOutput: 1216.08\tActual: -1216.12\n",
      "Input: [ 6.0142e+04 -1.3000e+01]\tOutput: 4626.17\tActual: -4626.31\n",
      "Input: [ 9.9517e+04 -5.7000e+01]\tOutput: 1745.87\tActual: -1745.91\n",
      "Input: [ 9.8355e+04 -2.4000e+01]\tOutput: 4098.00\tActual: -4098.12\n",
      "Input: [59866.   -79.]\tOutput: 757.78\tActual: -757.80\n",
      "Input: [54531.   -58.]\tOutput: 940.17\tActual: -940.19\n",
      "Input: [ 9.5454e+04 -6.4000e+01]\tOutput: 1491.43\tActual: -1491.47\n",
      "Input: [ 8.8098e+04 -4.0000e+01]\tOutput: 2202.39\tActual: -2202.45\n",
      "Input: [ 8.8278e+04 -4.0000e+00]\tOutput: 22068.71\tActual: -22069.50\n",
      "Input: [ 7.8138e+04 -4.5000e+01]\tOutput: 1736.35\tActual: -1736.40\n",
      "Input: [77065.   -98.]\tOutput: 786.36\tActual: -786.38\n",
      "Input: [ 7.9956e+04 -3.8000e+01]\tOutput: 2104.05\tActual: -2104.11\n",
      "Input: [ 9.1732e+04 -8.2000e+01]\tOutput: 1118.65\tActual: -1118.68\n",
      "Input: [ 8.9261e+04 -7.8000e+01]\tOutput: 1144.34\tActual: -1144.37\n",
      "Input: [60759.   -72.]\tOutput: 843.86\tActual: -843.88\n",
      "Input: [55439.   -71.]\tOutput: 780.81\tActual: -780.83\n",
      "Input: [ 9.7241e+04 -6.2000e+01]\tOutput: 1568.36\tActual: -1568.40\n",
      "Input: [ 9.2533e+04 -7.6000e+01]\tOutput: 1217.51\tActual: -1217.54\n",
      "Input: [78270.   -93.]\tOutput: 841.59\tActual: -841.61\n",
      "Input: [ 6.3279e+04 -4.3000e+01]\tOutput: 1471.57\tActual: -1471.60\n",
      "Input: [ 8.9279e+04 -8.5000e+01]\tOutput: 1050.32\tActual: -1050.34\n",
      "Input: [ 9.4251e+04 -2.9000e+01]\tOutput: 3249.94\tActual: -3250.03\n",
      "Input: [73888.  -100.]\tOutput: 738.86\tActual: -738.88\n",
      "Input: [67630.   -96.]\tOutput: 704.46\tActual: -704.48\n",
      "Input: [85144.   -98.]\tOutput: 868.80\tActual: -868.82\n",
      "Input: [ 8.1182e+04 -3.6000e+01]\tOutput: 2255.00\tActual: -2255.06\n",
      "Input: [75538.   -97.]\tOutput: 778.72\tActual: -778.74\n",
      "Input: [ 6.1824e+04 -2.0000e+00]\tOutput: 30910.88\tActual: -30912.00\n",
      "Input: [ 8.2125e+04 -5.0000e+00]\tOutput: 16424.44\tActual: -16425.00\n",
      "Input: [ 5.6979e+04 -3.7000e+01]\tOutput: 1539.93\tActual: -1539.97\n",
      "Input: [ 7.6189e+04 -3.0000e+00]\tOutput: 25395.43\tActual: -25396.33\n",
      "Input: [ 9.26e+04 -1.20e+01]\tOutput: 7716.42\tActual: -7716.67\n",
      "Input: [62504.   -91.]\tOutput: 686.84\tActual: -686.86\n",
      "Input: [70669.   -80.]\tOutput: 883.34\tActual: -883.36\n",
      "Input: [ 5.691e+04 -5.500e+01]\tOutput: 1034.70\tActual: -1034.73\n",
      "Input: [ 9.6857e+04 -1.0000e+01]\tOutput: 9685.38\tActual: -9685.70\n",
      "Input: [ 8.4964e+04 -7.4000e+01]\tOutput: 1148.13\tActual: -1148.16\n",
      "Input: [ 7.0131e+04 -3.5000e+01]\tOutput: 2003.69\tActual: -2003.74\n",
      "Input: [ 7.5227e+04 -6.0000e+01]\tOutput: 1253.75\tActual: -1253.78\n",
      "Input: [55978.   -88.]\tOutput: 636.10\tActual: -636.11\n",
      "Input: [71991.   -92.]\tOutput: 782.49\tActual: -782.51\n",
      "Input: [ 6.7153e+04 -6.5000e+01]\tOutput: 1033.10\tActual: -1033.12\n",
      "Input: [ 8.3548e+04 -1.5000e+01]\tOutput: 5569.70\tActual: -5569.87\n",
      "Input: [ 9.2676e+04 -1.1000e+01]\tOutput: 8424.82\tActual: -8425.09\n",
      "Input: [ 8.9071e+04 -4.3000e+01]\tOutput: 2071.36\tActual: -2071.42\n",
      "Input: [ 9.7454e+04 -4.7000e+01]\tOutput: 2073.43\tActual: -2073.49\n",
      "Input: [53983.   -89.]\tOutput: 606.54\tActual: -606.55\n",
      "Input: [ 5.9503e+04 -2.0000e+01]\tOutput: 2975.07\tActual: -2975.15\n",
      "Input: [ 7.9921e+04 -2.3000e+01]\tOutput: 3474.73\tActual: -3474.83\n",
      "Input: [66905.   -87.]\tOutput: 769.00\tActual: -769.02\n",
      "Input: [ 9.5838e+04 -6.7000e+01]\tOutput: 1430.38\tActual: -1430.42\n",
      "Input: [ 7.3953e+04 -4.8000e+01]\tOutput: 1540.65\tActual: -1540.69\n",
      "Input: [76806.   -81.]\tOutput: 948.20\tActual: -948.22\n",
      "Input: [ 9.5e+04 -2.6e+01]\tOutput: 3653.74\tActual: -3653.85\n",
      "Input: [72168.   -85.]\tOutput: 849.02\tActual: -849.04\n",
      "Input: [ 5.4583e+04 -6.0000e+00]\tOutput: 9096.87\tActual: -9097.17\n",
      "Input: [ 8.8714e+04 -3.1000e+01]\tOutput: 2861.66\tActual: -2861.74\n",
      "Input: [ 9.9145e+04 -4.5000e+01]\tOutput: 2203.16\tActual: -2203.22\n",
      "Input: [ 8.9896e+04 -1.8000e+01]\tOutput: 4994.07\tActual: -4994.22\n",
      "Input: [ 6.1889e+04 -4.3000e+01]\tOutput: 1439.24\tActual: -1439.28\n",
      "Input: [77413.   -97.]\tOutput: 798.05\tActual: -798.07\n",
      "Input: [62275.   -97.]\tOutput: 642.00\tActual: -642.01\n",
      "Input: [52461.   -81.]\tOutput: 647.65\tActual: -647.67\n",
      "Input: [64291.   -91.]\tOutput: 706.48\tActual: -706.49\n",
      "Input: [ 9.6804e+04 -7.7000e+01]\tOutput: 1257.16\tActual: -1257.19\n",
      "Input: [ 6.1435e+04 -2.0000e+00]\tOutput: 30716.37\tActual: -30717.50\n",
      "Input: [ 7.7935e+04 -7.5000e+01]\tOutput: 1039.11\tActual: -1039.13\n",
      "Input: [ 8.3453e+04 -6.4000e+01]\tOutput: 1303.92\tActual: -1303.95\n",
      "Input: [ 6.8544e+04 -1.6000e+01]\tOutput: 4283.87\tActual: -4284.00\n",
      "Input: [ 6.2258e+04 -4.7000e+01]\tOutput: 1324.60\tActual: -1324.64\n",
      "Input: [ 8.0841e+04 -8.0000e+01]\tOutput: 1010.49\tActual: -1010.51\n",
      "Input: [ 9.8758e+04 -2.7000e+01]\tOutput: 3657.60\tActual: -3657.70\n",
      "Input: [ 9.919e+04 -6.300e+01]\tOutput: 1574.40\tActual: -1574.44\n",
      "Input: [ 6.0736e+04 -5.5000e+01]\tOutput: 1104.26\tActual: -1104.29\n",
      "Input: [87643.   -97.]\tOutput: 903.51\tActual: -903.54\n",
      "Input: [ 9.395e+04 -4.100e+01]\tOutput: 2291.40\tActual: -2291.46\n",
      "Input: [ 8.4964e+04 -4.4000e+01]\tOutput: 1930.95\tActual: -1931.00\n",
      "Input: [ 7.8044e+04 -5.6000e+01]\tOutput: 1393.61\tActual: -1393.64\n",
      "Input: [ 8.9933e+04 -8.1000e+01]\tOutput: 1110.26\tActual: -1110.28\n",
      "Input: [65997.   -84.]\tOutput: 785.66\tActual: -785.68\n",
      "Input: [ 6.4215e+04 -3.0000e+01]\tOutput: 2140.44\tActual: -2140.50\n",
      "Input: [83069.   -97.]\tOutput: 856.36\tActual: -856.38\n",
      "Input: [ 7.3911e+04 -8.0000e+00]\tOutput: 9238.58\tActual: -9238.88\n",
      "Input: [67030.   -87.]\tOutput: 770.44\tActual: -770.46\n",
      "Input: [79334.   -98.]\tOutput: 809.51\tActual: -809.53\n",
      "Input: [ 6.4571e+04 -1.3000e+01]\tOutput: 4966.84\tActual: -4967.00\n",
      "Input: [ 9.2719e+04 -2.1000e+01]\tOutput: 4415.05\tActual: -4415.19\n",
      "Input: [ 7.8927e+04 -4.3000e+01]\tOutput: 1835.46\tActual: -1835.51\n",
      "Input: [ 7.0714e+04 -4.6000e+01]\tOutput: 1537.22\tActual: -1537.26\n",
      "Input: [93541.   -97.]\tOutput: 964.32\tActual: -964.34\n",
      "Input: [73387.   -77.]\tOutput: 953.05\tActual: -953.08\n",
      "Input: [ 7.8444e+04 -6.7000e+01]\tOutput: 1170.78\tActual: -1170.81\n",
      "Input: [ 9.8908e+04 -7.1000e+01]\tOutput: 1393.04\tActual: -1393.07\n",
      "Input: [ 6.9921e+04 -1.9000e+01]\tOutput: 3679.94\tActual: -3680.05\n",
      "Input: [63057.   -82.]\tOutput: 768.97\tActual: -768.99\n",
      "Input: [ 9.5268e+04 -7.0000e+01]\tOutput: 1360.94\tActual: -1360.97\n",
      "Input: [ 7.4858e+04 -4.1000e+01]\tOutput: 1825.76\tActual: -1825.80\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_test))[:100]:\n",
    "    print(f\"Input: {x_test[i]}\\tOutput: {y_test_pred[i].numpy()[0]:.2f}\\tActual: {y_test[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The absolute value are approximately correct, however, it couldn't predict negative values as there were no negative values in the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
