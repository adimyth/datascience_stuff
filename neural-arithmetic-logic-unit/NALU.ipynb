{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Arithmetic Logic Unit (NALU)\n",
    "\n",
    "NALU combines Simple NAC & Complex NAC to perform any type of arithmetic operation. NALU comprises the weighted combination of a simple NAC and a complex NAC mentioned above, controlled by a learned gate signal.\n",
    "\n",
    "$$y = g*a+(1-g)*m$$\n",
    "where,\n",
    "$a = Wx$\n",
    "\n",
    "$m = \\exp [W(\\log(|x|+\\varepsilon))]$\n",
    "\n",
    "$W= \\tanh(\\hat{W})\\odot \\sigma (\\hat{M})$\n",
    "\n",
    "$g = sigmoid(Gx)$, G is a trainable parameter\n",
    "\n",
    "In the above equation, $a$ represents the Simple NAC layer & $m$ represents the Complex NAC layer. See, the other two notebooks in this directory for reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Equations\n",
    "\n",
    "Consider, the following equation -\n",
    "$$y = 13a^{4}+5b^{3}-7c^{2}+2d$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equate(a, b, c, d):\n",
    "    return float(13*(a**4)+5*(b**3)-7*(c**2)+2*d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randint(low=10, high=10000, size=1000)\n",
    "b = np.random.randint(low=10, high=10000, size=1000)\n",
    "c = np.random.randint(low=10, high=10000, size=1000)\n",
    "d = np.random.randint(low=10, high=10000, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.column_stack((a, b, c, d))\n",
    "x_train = x_train.astype('float32')\n",
    "y_train = [equate(x[0], x[1], x[2], x[3]) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.random.randint(low=10, high=100, size=1000)\n",
    "b1 = np.random.randint(low=10, high=100, size=1000)\n",
    "c1 = np.random.randint(low=10, high=100, size=1000)\n",
    "d1 = np.random.randint(low=10, high=100, size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.column_stack((a1, b1, c1, d1))\n",
    "x_test = x_test.astype('float32')\n",
    "y_test = [equate(x[0], x[1], x[2], x[3]) for x in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle_loss(predicted_y, target_y, epsilon=0.000001):\n",
    "    predicted_y = tf.squeeze(predicted_y)\n",
    "    predicted_y = [x+epsilon for x in predicted_y]\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log(predicted_y) - tf.math.log(target_y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_loss(predicted_y, target_y, epsilon=0.000001):\n",
    "    predicted_y = tf.squeeze(predicted_y)\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(predicted_y - target_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NALU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000\n",
    "alpha = 0.1\n",
    "epsilon = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_simple_hat = tf.Variable(shape=[4, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[4, 1], minval=0, maxval=2),\n",
    "                        trainable=True, name=\"W_hat\")\n",
    "M_simple_hat = tf.Variable(shape=[4, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[4, 1], minval=0, maxval=2),\n",
    "                        trainable=True, name=\"M_hat\")\n",
    "W_complex_hat = tf.Variable(shape=[4, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[4, 1], minval=0, maxval=2),\n",
    "                        trainable=True, name=\"W_hat\")\n",
    "M_complex_hat = tf.Variable(shape=[4, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[4, 1], minval=0, maxval=2),\n",
    "                        trainable=True, name=\"M_hat\")\n",
    "# gate signal layer\n",
    "G = tf.Variable(shape=[4, 1],\n",
    "                    initial_value=tf.random.uniform(shape=[4, 1], minval=0, maxval=2),\n",
    "                    trainable=True, name=\"Gate_weights\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 26.651687622070312\n"
     ]
    }
   ],
   "source": [
    "# simple nac layer output\n",
    "W_simple = tf.nn.tanh(W_simple_hat) * tf.nn.sigmoid(M_simple_hat) \n",
    "y_simple_pred = tf.matmul(x_train, W_simple)\n",
    "\n",
    "# complex nac layer output\n",
    "W_complex = tf.nn.tanh(W_complex_hat) * tf.nn.sigmoid(M_complex_hat) \n",
    "x_modified = tf.math.log(tf.abs(x_train) + epsilon)\n",
    "y_complex_pred = tf.exp(tf.matmul(x_modified, W_complex))\n",
    "\n",
    "# nalu output\n",
    "g = tf.nn.sigmoid(tf.matmul(x_train, G))\n",
    "y_out = g*y_simple_pred + (1-g)*y_complex_pred\n",
    "print(f\"Initial Loss: {rmsle_loss(y_out, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = tf.keras.optimizers.Adam(learning_rate=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, learning_rate=0.01):\n",
    "    with tf.GradientTape() as t:\n",
    "        # update weight matrix (W) -> restricted to [-1, 1]\n",
    "        W_simple = tf.nn.tanh(W_simple_hat) * tf.nn.sigmoid(M_simple_hat) \n",
    "        # simple nac layer output\n",
    "        y_simple_pred = tf.matmul(X, W_simple)\n",
    "\n",
    "        # complex nac layer output\n",
    "        W_complex = tf.nn.tanh(W_complex_hat) * tf.nn.sigmoid(M_complex_hat) \n",
    "        x_modified = tf.math.log(tf.abs(X) + epsilon)\n",
    "        y_complex_pred = tf.exp(tf.matmul(x_modified, W_complex))\n",
    "\n",
    "        # nalu output\n",
    "        g = tf.nn.sigmoid(tf.matmul(X, G))\n",
    "        y_out = g*y_simple_pred + (1-g)*y_complex_pred\n",
    "\n",
    "        # calculate loss\n",
    "        current_loss = rmsle_loss(y_out, y)\n",
    "    dW_simple_hat, dM_simple_hat, dW_complex_hat, dM_complex_hat, dG = t.gradient(current_loss, (W_simple_hat, M_simple_hat, W_complex_hat, M_complex_hat, G))\n",
    "    optimize.apply_gradients(zip([W_simple_hat, dM_simple_hat, dW_complex_hat, dM_complex_hat, dG], [W_simple_hat, M_simple_hat, W_complex_hat, M_complex_hat, G]))\n",
    "    return current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 26.65\n",
      "Epoch: 500\tLoss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-136c5e06c756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch}\\tLoss: {current_loss:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-1819dc96ee0b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(X, y, learning_rate)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mcurrent_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrmsle_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdW_simple_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdM_simple_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_complex_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdM_complex_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW_simple_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_simple_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_complex_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_complex_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mW_simple_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdM_simple_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW_complex_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdM_complex_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdG\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mW_simple_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_simple_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_complex_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM_complex_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcurrent_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mgradients\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m   \u001b[0mmock_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MockOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m   \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_name\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, attrs, inputs, outputs, typ, skip_input_indices)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;34m\"\"\"Pretends to be a tf.Operation for the gradient functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    current_loss = train(x_train, y_train)\n",
    "    loss_history.append(current_loss)\n",
    "    if epoch%500==0:\n",
    "        print(f\"Epoch: {epoch}\\tLoss: {current_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1392976d8>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(X):\n",
    "    y_simple_pred = tf.matmul(X, W)\n",
    "    x_modified = tf.math.log(tf.abs(X) + epsilon)\n",
    "    y_complex_pred = tf.exp(tf.matmul(x_modified, W))\n",
    "    g = tf.nn.sigmoid(tf.matmul(X, G))\n",
    "    y_out = g*y_simple_pred + (1-g)*y_complex_pred\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = get_pred(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2130835"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13*(14**4)+5*(69**3)-7*(40**2)+2*41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [14. 69. 40. 41.]\tOutput: 48.98\tActual: 2130835.0\n",
      "Input: [57. 69. 23. 71.]\tOutput: 80.15\tActual: 138866997.0\n",
      "Input: [74. 75. 60. 37.]\tOutput: 88.13\tActual: 391909737.0\n",
      "Input: [13. 46. 40. 72.]\tOutput: 53.83\tActual: 846917.0\n",
      "Input: [26. 89. 25. 78.]\tOutput: 70.25\tActual: 9461314.0\n",
      "Input: [43. 97. 35. 41.]\tOutput: 71.17\tActual: 48999285.0\n",
      "Input: [68. 31. 17. 55.]\tOutput: 70.07\tActual: 278104930.0\n",
      "Input: [65. 91. 76. 10.]\tOutput: 81.12\tActual: 235785568.0\n",
      "Input: [85. 55. 61. 23.]\tOutput: 84.82\tActual: 679413999.0\n",
      "Input: [96. 69. 10. 99.]\tOutput: 109.28\tActual: 1105792571.0\n",
      "Input: [47. 60. 97. 49.]\tOutput: 82.20\tActual: 64450088.0\n",
      "Input: [46. 74. 87. 46.]\tOutput: 81.57\tActual: 60180157.0\n",
      "Input: [95. 12. 87. 63.]\tOutput: 100.55\tActual: 1058813908.0\n",
      "Input: [56. 68. 56. 36.]\tOutput: 74.82\tActual: 129398728.0\n",
      "Input: [64. 61. 38. 90.]\tOutput: 92.57\tActual: 219228785.0\n",
      "Input: [17. 22. 67. 97.]\tOutput: 65.66\tActual: 1107784.0\n",
      "Input: [10. 94. 26. 56.]\tOutput: 54.74\tActual: 4278300.0\n",
      "Input: [92. 28. 80. 20.]\tOutput: 85.76\tActual: 931375848.0\n",
      "Input: [17. 24. 17. 56.]\tOutput: 39.55\tActual: 1152982.0\n",
      "Input: [45. 40. 57. 85.]\tOutput: 79.38\tActual: 53605552.0\n",
      "Input: [19. 38. 27. 72.]\tOutput: 52.19\tActual: 1963574.0\n",
      "Input: [59. 24. 87. 87.]\tOutput: 91.42\tActual: 157542004.0\n",
      "Input: [73. 84. 12. 47.]\tOutput: 81.77\tActual: 372139739.0\n",
      "Input: [24. 93. 83. 56.]\tOutput: 76.20\tActual: 8286762.0\n",
      "Input: [74. 38. 55. 31.]\tOutput: 75.80\tActual: 390078735.0\n",
      "Input: [36. 76. 23. 55.]\tOutput: 64.15\tActual: 24026295.0\n",
      "Input: [28. 24. 69. 22.]\tOutput: 46.25\tActual: 8026365.0\n",
      "Input: [67. 65. 88. 99.]\tOutput: 110.47\tActual: 263283688.0\n",
      "Input: [84. 49. 51. 17.]\tOutput: 78.26\tActual: 647802840.0\n",
      "Input: [60. 40. 28. 91.]\tOutput: 83.12\tActual: 168794694.0\n",
      "Input: [46. 78. 86. 98.]\tOutput: 100.79\tActual: 60528112.0\n",
      "Input: [28. 17. 48. 95.]\tOutput: 65.46\tActual: 7999155.0\n",
      "Input: [13. 85. 46. 63.]\tOutput: 61.56\tActual: 3427232.0\n",
      "Input: [18. 98. 69. 70.]\tOutput: 75.60\tActual: 6037461.0\n",
      "Input: [53. 43. 42. 58.]\tOutput: 71.48\tActual: 102961556.0\n",
      "Input: [36. 96. 36. 39.]\tOutput: 66.45\tActual: 26249694.0\n",
      "Input: [47. 95. 57. 77.]\tOutput: 91.06\tActual: 67700139.0\n",
      "Input: [42. 39. 63. 42.]\tOutput: 63.58\tActual: 40720944.0\n",
      "Input: [21. 96. 97. 83.]\tOutput: 88.18\tActual: 6886236.0\n",
      "Input: [98. 69. 67. 65.]\tOutput: 112.04\tActual: 1200689860.0\n",
      "Input: [40. 80. 10. 29.]\tOutput: 55.05\tActual: 35839358.0\n",
      "Input: [77. 66. 56. 81.]\tOutput: 102.34\tActual: 458405223.0\n",
      "Input: [95. 18. 59. 28.]\tOutput: 82.84\tActual: 1058862974.0\n",
      "Input: [77. 86. 70. 17.]\tOutput: 87.82\tActual: 460135547.0\n",
      "Input: [35. 22. 43. 46.]\tOutput: 52.05\tActual: 19548514.0\n",
      "Input: [97. 33. 32. 48.]\tOutput: 88.25\tActual: 1151053266.0\n",
      "Input: [26. 51. 44. 23.]\tOutput: 46.02\tActual: 6590437.0\n",
      "Input: [62. 24. 69. 31.]\tOutput: 68.90\tActual: 192128223.0\n",
      "Input: [11. 27. 63. 69.]\tOutput: 52.53\tActual: 261103.0\n",
      "Input: [14. 24. 46. 68.]\tOutput: 49.07\tActual: 553852.0\n",
      "Input: [95. 72. 44. 75.]\tOutput: 109.08\tActual: 1060710963.0\n",
      "Input: [59. 92. 88. 14.]\tOutput: 82.24\tActual: 161364953.0\n",
      "Input: [11. 11. 30. 38.]\tOutput: 29.69\tActual: 190764.0\n",
      "Input: [42. 54. 24. 80.]\tOutput: 71.36\tActual: 41235496.0\n",
      "Input: [96. 76. 11. 66.]\tOutput: 99.49\tActual: 1106344693.0\n",
      "Input: [69. 99. 95. 26.]\tOutput: 95.61\tActual: 299460945.0\n",
      "Input: [86. 13. 81. 16.]\tOutput: 77.50\tActual: 711075698.0\n",
      "Input: [85. 98. 86. 40.]\tOutput: 107.33\tActual: 683262393.0\n",
      "Input: [38. 83. 77. 34.]\tOutput: 72.51\tActual: 29924268.0\n",
      "Input: [23. 95. 71. 20.]\tOutput: 60.43\tActual: 7889561.0\n",
      "Input: [41. 21. 38. 64.]\tOutput: 60.44\tActual: 36771218.0\n",
      "Input: [90. 39. 27. 66.]\tOutput: 90.91\tActual: 853221624.0\n",
      "Input: [38. 24. 64. 21.]\tOutput: 50.42\tActual: 27147258.0\n",
      "Input: [88. 99. 61. 85.]\tOutput: 119.28\tActual: 784429586.0\n",
      "Input: [18. 58. 60. 32.]\tOutput: 50.20\tActual: 2315112.0\n",
      "Input: [12. 54. 18. 25.]\tOutput: 33.21\tActual: 1054670.0\n",
      "Input: [54. 59. 58. 83.]\tOutput: 88.68\tActual: 111543241.0\n",
      "Input: [37. 16. 66. 39.]\tOutput: 54.78\tActual: 24354159.0\n",
      "Input: [96. 56. 41. 95.]\tOutput: 112.15\tActual: 1105017031.0\n",
      "Input: [46. 79. 14. 30.]\tOutput: 59.55\tActual: 60670811.0\n",
      "Input: [85. 77. 94. 65.]\tOutput: 113.04\tActual: 680829068.0\n",
      "Input: [14. 86. 44. 73.]\tOutput: 65.45\tActual: 3666282.0\n",
      "Input: [27. 51. 15. 28.]\tOutput: 41.40\tActual: 7570469.0\n",
      "Input: [91. 28. 62. 54.]\tOutput: 92.95\tActual: 891557453.0\n",
      "Input: [45. 32. 63. 51.]\tOutput: 66.79\tActual: 53444284.0\n",
      "Input: [29. 21. 93. 59.]\tOutput: 65.02\tActual: 9180533.0\n",
      "Input: [18. 90. 27. 80.]\tOutput: 67.11\tActual: 5004745.0\n",
      "Input: [81. 24. 92. 67.]\tOutput: 98.09\tActual: 559617379.0\n",
      "Input: [10. 18. 19. 80.]\tOutput: 43.10\tActual: 156793.0\n",
      "Input: [83. 47. 24. 24.]\tOutput: 73.20\tActual: 617473304.0\n",
      "Input: [88. 47. 70. 36.]\tOutput: 91.38\tActual: 780088855.0\n",
      "Input: [78. 38. 37. 25.]\tOutput: 71.63\tActual: 481460555.0\n",
      "Input: [50. 66. 63. 28.]\tOutput: 69.74\tActual: 82659753.0\n",
      "Input: [61. 50. 76. 54.]\tOutput: 84.51\tActual: 180580609.0\n",
      "Input: [52. 53. 23. 24.]\tOutput: 56.69\tActual: 95791738.0\n",
      "Input: [37. 19. 76. 12.]\tOutput: 48.31\tActual: 24357980.0\n",
      "Input: [97. 87. 87. 29.]\tOutput: 107.85\tActual: 1154120243.0\n",
      "Input: [61. 36. 81. 76.]\tOutput: 90.13\tActual: 180183438.0\n",
      "Input: [65. 32. 78. 97.]\tOutput: 98.19\tActual: 232179571.0\n",
      "Input: [37. 90. 29. 26.]\tOutput: 59.26\tActual: 28003258.0\n",
      "Input: [14. 46. 16. 61.]\tOutput: 44.73\tActual: 984418.0\n",
      "Input: [23. 95. 77. 20.]\tOutput: 61.87\tActual: 7883345.0\n",
      "Input: [54. 42. 44. 60.]\tOutput: 73.00\tActual: 110896736.0\n",
      "Input: [87. 16. 40. 91.]\tOutput: 95.61\tActual: 744776355.0\n",
      "Input: [90. 82. 25. 10.]\tOutput: 80.98\tActual: 855682485.0\n",
      "Input: [84. 34. 13. 58.]\tOutput: 80.05\tActual: 647428221.0\n",
      "Input: [92. 63. 50. 71.]\tOutput: 105.19\tActual: 932543725.0\n",
      "Input: [28. 22. 14. 28.]\tOutput: 34.68\tActual: 8042452.0\n",
      "Input: [23. 84. 97. 56.]\tOutput: 76.80\tActual: 6535702.0\n",
      "Input: [57. 81. 12. 50.]\tOutput: 72.96\tActual: 139884310.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_test))[:100]:\n",
    "    print(f\"Input: {x_test[i]}\\tOutput: {y_test_pred[i].numpy()[0]:.2f}\\tActual: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0. 5.]\tOutput: 5.00\tActual: 5.0\n",
      "Input: [ 5. 10.]\tOutput: 15.00\tActual: 15.0\n",
      "Input: [10. 15.]\tOutput: 25.00\tActual: 25.0\n",
      "Input: [15. 20.]\tOutput: 35.00\tActual: 35.0\n",
      "Input: [20. 25.]\tOutput: 45.00\tActual: 45.0\n",
      "Input: [25. 30.]\tOutput: 55.00\tActual: 55.0\n",
      "Input: [30. 35.]\tOutput: 65.00\tActual: 65.0\n",
      "Input: [35. 40.]\tOutput: 75.00\tActual: 75.0\n",
      "Input: [40. 45.]\tOutput: 84.99\tActual: 85.0\n",
      "Input: [45. 50.]\tOutput: 94.99\tActual: 95.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Input: {x_train[i]}\\tOutput: {tf.matmul([x_train[i]], W)[0][0]:.2f}\\tActual: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
