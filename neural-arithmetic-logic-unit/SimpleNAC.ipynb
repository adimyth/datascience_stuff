{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex NAC Layer\n",
    "\n",
    "$y=\\exp [W(\\log(|x|+\\varepsilon))]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a complex NAC in log space\n",
    "# Usefulfor more complex arithmetic functions such as multiplication, division and power\n",
    "\n",
    "def nac_complex_single_layer(x_in, out_units, epsilon = 0.000001):\n",
    "    '''\n",
    "    Attributes\n",
    "    -----------\n",
    "    x_in -> Input tensor\n",
    "    out_units -> number of output units\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "    y_out -> Output tensor of mentioned shape\n",
    "    W -> Weight matrix of the layer\n",
    "    '''\n",
    "    \n",
    "    # Get the number of input features (numbers)\n",
    "    in_features = x_in.shape[1]\n",
    "\n",
    "    # random initialier\n",
    "    x = tf.initializers.RandomUniform(minval=-2, maxval=2)\n",
    "    \n",
    "    # defining W_hat and M_hat\n",
    "    W_hat = tf.Variable(shape=[in_features, out_units],\n",
    "                        initial_value=x.__call__(shape=[in_features, out_units]), \n",
    "                        trainable=True, name=\"W_hat\")\n",
    "\n",
    "    M_hat = tf.Variable(shape=[in_features, out_units],\n",
    "                        initial_value=x.__call__(shape=[in_features, out_units]), \n",
    "                        trainable=True, name=\"M_hat\")\n",
    "    \n",
    "    # Weight Matrix (W) -> restricted to [-1, 1]\n",
    "    W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "\n",
    "    # Express input feature in log space to learn complex functions\n",
    "    x_modified = tf.log(tf.abs(x_in) + epsilon)\n",
    "\n",
    "    y = tf.exp( tf.matmul(x_modified, W) )\n",
    "\n",
    "    return y, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Arithmetic Logic Unit (NALU)\n",
    "\n",
    "NALU combines Simple NAC & Complex NAC to perform any type of arithmetic operation. NALU comprises the weighted combination of a simple NAC and a complex NAC mentioned above, controlled by a learned gate signal.\n",
    "\n",
    "$$y = g*a+(1-g)*m$$\n",
    "where,\n",
    "$a = Wx$\n",
    "\n",
    "$m = \\exp [W(\\log(|x|+\\varepsilon))]$\n",
    "\n",
    "$W= \\tanh(\\hat{W})\\odot \\sigma (\\hat{M})$\n",
    "\n",
    "$g = sigmoid(Gx)$, G is a trainable parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nalu(x_in, out_units, epsilon=0.000001, get_weights=False):\n",
    "    '''\n",
    "    Attributes\n",
    "    -----------\n",
    "    x_in -> Input tensor\n",
    "    out_units -> number of output units\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "    y_out -> Output tensor of mentioned shape\n",
    "    '''\n",
    "\n",
    "    in_features = x_in.shape[1]\n",
    "\n",
    "    # get the output tensor from simple NAC\n",
    "    a, W_simple = nac_simple_single_layer(x_in, out_units)\n",
    "\n",
    "    # get the output tensor from complex NAC\n",
    "    m, W_complex = nac_complex_single_layer(x_in, out_units, epsilon= epsilon)\n",
    "    \n",
    "    # In the paper, the authors use the same weights for complex as well as \n",
    "    # simple NAC i.e W_simple = W_complex\n",
    "    \n",
    "    # random initialier\n",
    "    x = tf.initializers.RandomUniform(minval=-2, maxval=2)\n",
    "\n",
    "    # gate signal layer\n",
    "    G = tf.Variable(shape=[in_features, out_units],\n",
    "                    initial_value=x.__call__(shape=(2, 2)),\n",
    "                    trainable=True, name=\"Gate_weights\")\n",
    "\n",
    "    g = tf.nn.sigmoid(tf.matmul(x_in, G))\n",
    "\n",
    "    y_out = g * a + (1 - g) * m\n",
    "\n",
    "    return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple NAC\n",
    "\n",
    "Here, I test on addition operation using simple NAC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a series of input number X1 and X2 for training\n",
    "x1 = np.arange(0,10000,5, dtype=np.float32)\n",
    "x2 = np.arange(5,10005,5, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.column_stack((x1,x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating testing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a series of input number X1 and X2 for testing\n",
    "x1 = np.arange(1000,2000,8, dtype=np.float32)\n",
    "x2 = np.arange(1000,1500,4, dtype= np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.column_stack((x1,x2))\n",
    "y_test = x1 + x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 2)\n",
      "(125,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(predicted_y, target_y):\n",
    "    predicted_y = tf.squeeze(predicted_y)\n",
    "    return tf.reduce_mean(tf.square(predicted_y - target_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Neural Accumulator (NAC) Layer**\n",
    "\n",
    "$W= \\tanh(\\hat{W})\\odot \\sigma (\\hat{M})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20000\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat = tf.Variable(shape=[2, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[2, 1], minval=-2, maxval=2),\n",
    "                        trainable=True, name=\"W_hat\")\n",
    "M_hat = tf.Variable(shape=[2, 1],\n",
    "                        initial_value=tf.random.uniform(shape=[2, 1], minval=-2, maxval=2),\n",
    "                        trainable=True, name=\"M_hat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = tf.keras.optimizers.Adam(learning_rate=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Loss: 264138736.0\n"
     ]
    }
   ],
   "source": [
    "W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "y_pred = tf.matmul(x_train, W)\n",
    "print(f\"Initial Loss: {loss(y_pred, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, learning_rate=0.01):\n",
    "    with tf.GradientTape() as t:\n",
    "        # update weight matrix (W) -> restricted to [-1, 1]\n",
    "        W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "        # make prediction\n",
    "        y_pred = tf.matmul(X, W)\n",
    "        # calculate loss\n",
    "        current_loss = loss(y_pred, y)\n",
    "    dW_hat, dM_hat = t.gradient(current_loss, (W_hat, M_hat))\n",
    "    optimize.apply_gradients(zip([dW_hat, dM_hat], [W_hat, M_hat]))\n",
    "    return current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 264138736.00\n",
      "Epoch: 500\tLoss: 67368.75\n",
      "Epoch: 1000\tLoss: 23683.92\n",
      "Epoch: 1500\tLoss: 12110.54\n",
      "Epoch: 2000\tLoss: 7242.39\n",
      "Epoch: 2500\tLoss: 4714.86\n",
      "Epoch: 3000\tLoss: 3233.80\n",
      "Epoch: 3500\tLoss: 2295.61\n",
      "Epoch: 4000\tLoss: 1669.04\n",
      "Epoch: 4500\tLoss: 1234.37\n",
      "Epoch: 5000\tLoss: 924.35\n",
      "Epoch: 5500\tLoss: 698.55\n",
      "Epoch: 6000\tLoss: 531.64\n",
      "Epoch: 6500\tLoss: 406.71\n",
      "Epoch: 7000\tLoss: 312.39\n",
      "Epoch: 7500\tLoss: 240.66\n",
      "Epoch: 8000\tLoss: 185.86\n",
      "Epoch: 8500\tLoss: 143.83\n",
      "Epoch: 9000\tLoss: 111.43\n",
      "Epoch: 9500\tLoss: 86.43\n",
      "Epoch: 10000\tLoss: 67.10\n",
      "Epoch: 10500\tLoss: 52.14\n",
      "Epoch: 11000\tLoss: 40.52\n",
      "Epoch: 11500\tLoss: 31.52\n",
      "Epoch: 12000\tLoss: 24.51\n",
      "Epoch: 12500\tLoss: 19.06\n",
      "Epoch: 13000\tLoss: 14.84\n",
      "Epoch: 13500\tLoss: 11.54\n",
      "Epoch: 14000\tLoss: 8.99\n",
      "Epoch: 14500\tLoss: 7.00\n",
      "Epoch: 15000\tLoss: 5.44\n",
      "Epoch: 15500\tLoss: 4.24\n",
      "Epoch: 16000\tLoss: 3.30\n",
      "Epoch: 16500\tLoss: 2.57\n",
      "Epoch: 17000\tLoss: 2.00\n",
      "Epoch: 17500\tLoss: 1.56\n",
      "Epoch: 18000\tLoss: 1.21\n",
      "Epoch: 18500\tLoss: 0.94\n",
      "Epoch: 19000\tLoss: 0.73\n",
      "Epoch: 19500\tLoss: 0.57\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    current_loss = train(x_train, y_train)\n",
    "    loss_history.append(current_loss)\n",
    "    if epoch%500==0:\n",
    "        print(f\"Epoch: {epoch}\\tLoss: {current_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1392af630>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history[4000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.nn.tanh(W_hat) * tf.nn.sigmoid(M_hat) \n",
    "y_pred = tf.matmul(x_test, W)\n",
    "current_loss = loss(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1080., 1040.], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1719718, shape=(1, 1), dtype=float32, numpy=array([[2119.878]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul([x_test[10]], W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2120.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [1000. 1000.]\tOutput: 1999.88\tActual: 2000.0\n",
      "Input: [1008. 1004.]\tOutput: 2011.88\tActual: 2012.0\n",
      "Input: [1016. 1008.]\tOutput: 2023.88\tActual: 2024.0\n",
      "Input: [1024. 1012.]\tOutput: 2035.88\tActual: 2036.0\n",
      "Input: [1032. 1016.]\tOutput: 2047.88\tActual: 2048.0\n",
      "Input: [1040. 1020.]\tOutput: 2059.88\tActual: 2060.0\n",
      "Input: [1048. 1024.]\tOutput: 2071.88\tActual: 2072.0\n",
      "Input: [1056. 1028.]\tOutput: 2083.88\tActual: 2084.0\n",
      "Input: [1064. 1032.]\tOutput: 2095.88\tActual: 2096.0\n",
      "Input: [1072. 1036.]\tOutput: 2107.88\tActual: 2108.0\n",
      "Input: [1080. 1040.]\tOutput: 2119.88\tActual: 2120.0\n",
      "Input: [1088. 1044.]\tOutput: 2131.88\tActual: 2132.0\n",
      "Input: [1096. 1048.]\tOutput: 2143.88\tActual: 2144.0\n",
      "Input: [1104. 1052.]\tOutput: 2155.88\tActual: 2156.0\n",
      "Input: [1112. 1056.]\tOutput: 2167.88\tActual: 2168.0\n",
      "Input: [1120. 1060.]\tOutput: 2179.88\tActual: 2180.0\n",
      "Input: [1128. 1064.]\tOutput: 2191.87\tActual: 2192.0\n",
      "Input: [1136. 1068.]\tOutput: 2203.87\tActual: 2204.0\n",
      "Input: [1144. 1072.]\tOutput: 2215.87\tActual: 2216.0\n",
      "Input: [1152. 1076.]\tOutput: 2227.87\tActual: 2228.0\n",
      "Input: [1160. 1080.]\tOutput: 2239.87\tActual: 2240.0\n",
      "Input: [1168. 1084.]\tOutput: 2251.87\tActual: 2252.0\n",
      "Input: [1176. 1088.]\tOutput: 2263.87\tActual: 2264.0\n",
      "Input: [1184. 1092.]\tOutput: 2275.87\tActual: 2276.0\n",
      "Input: [1192. 1096.]\tOutput: 2287.87\tActual: 2288.0\n",
      "Input: [1200. 1100.]\tOutput: 2299.87\tActual: 2300.0\n",
      "Input: [1208. 1104.]\tOutput: 2311.87\tActual: 2312.0\n",
      "Input: [1216. 1108.]\tOutput: 2323.87\tActual: 2324.0\n",
      "Input: [1224. 1112.]\tOutput: 2335.87\tActual: 2336.0\n",
      "Input: [1232. 1116.]\tOutput: 2347.87\tActual: 2348.0\n",
      "Input: [1240. 1120.]\tOutput: 2359.87\tActual: 2360.0\n",
      "Input: [1248. 1124.]\tOutput: 2371.86\tActual: 2372.0\n",
      "Input: [1256. 1128.]\tOutput: 2383.86\tActual: 2384.0\n",
      "Input: [1264. 1132.]\tOutput: 2395.86\tActual: 2396.0\n",
      "Input: [1272. 1136.]\tOutput: 2407.86\tActual: 2408.0\n",
      "Input: [1280. 1140.]\tOutput: 2419.86\tActual: 2420.0\n",
      "Input: [1288. 1144.]\tOutput: 2431.86\tActual: 2432.0\n",
      "Input: [1296. 1148.]\tOutput: 2443.86\tActual: 2444.0\n",
      "Input: [1304. 1152.]\tOutput: 2455.86\tActual: 2456.0\n",
      "Input: [1312. 1156.]\tOutput: 2467.86\tActual: 2468.0\n",
      "Input: [1320. 1160.]\tOutput: 2479.86\tActual: 2480.0\n",
      "Input: [1328. 1164.]\tOutput: 2491.86\tActual: 2492.0\n",
      "Input: [1336. 1168.]\tOutput: 2503.86\tActual: 2504.0\n",
      "Input: [1344. 1172.]\tOutput: 2515.86\tActual: 2516.0\n",
      "Input: [1352. 1176.]\tOutput: 2527.86\tActual: 2528.0\n",
      "Input: [1360. 1180.]\tOutput: 2539.86\tActual: 2540.0\n",
      "Input: [1368. 1184.]\tOutput: 2551.86\tActual: 2552.0\n",
      "Input: [1376. 1188.]\tOutput: 2563.85\tActual: 2564.0\n",
      "Input: [1384. 1192.]\tOutput: 2575.85\tActual: 2576.0\n",
      "Input: [1392. 1196.]\tOutput: 2587.85\tActual: 2588.0\n",
      "Input: [1400. 1200.]\tOutput: 2599.85\tActual: 2600.0\n",
      "Input: [1408. 1204.]\tOutput: 2611.85\tActual: 2612.0\n",
      "Input: [1416. 1208.]\tOutput: 2623.85\tActual: 2624.0\n",
      "Input: [1424. 1212.]\tOutput: 2635.85\tActual: 2636.0\n",
      "Input: [1432. 1216.]\tOutput: 2647.85\tActual: 2648.0\n",
      "Input: [1440. 1220.]\tOutput: 2659.85\tActual: 2660.0\n",
      "Input: [1448. 1224.]\tOutput: 2671.85\tActual: 2672.0\n",
      "Input: [1456. 1228.]\tOutput: 2683.85\tActual: 2684.0\n",
      "Input: [1464. 1232.]\tOutput: 2695.85\tActual: 2696.0\n",
      "Input: [1472. 1236.]\tOutput: 2707.85\tActual: 2708.0\n",
      "Input: [1480. 1240.]\tOutput: 2719.85\tActual: 2720.0\n",
      "Input: [1488. 1244.]\tOutput: 2731.85\tActual: 2732.0\n",
      "Input: [1496. 1248.]\tOutput: 2743.85\tActual: 2744.0\n",
      "Input: [1504. 1252.]\tOutput: 2755.84\tActual: 2756.0\n",
      "Input: [1512. 1256.]\tOutput: 2767.84\tActual: 2768.0\n",
      "Input: [1520. 1260.]\tOutput: 2779.84\tActual: 2780.0\n",
      "Input: [1528. 1264.]\tOutput: 2791.84\tActual: 2792.0\n",
      "Input: [1536. 1268.]\tOutput: 2803.84\tActual: 2804.0\n",
      "Input: [1544. 1272.]\tOutput: 2815.84\tActual: 2816.0\n",
      "Input: [1552. 1276.]\tOutput: 2827.84\tActual: 2828.0\n",
      "Input: [1560. 1280.]\tOutput: 2839.84\tActual: 2840.0\n",
      "Input: [1568. 1284.]\tOutput: 2851.84\tActual: 2852.0\n",
      "Input: [1576. 1288.]\tOutput: 2863.84\tActual: 2864.0\n",
      "Input: [1584. 1292.]\tOutput: 2875.84\tActual: 2876.0\n",
      "Input: [1592. 1296.]\tOutput: 2887.84\tActual: 2888.0\n",
      "Input: [1600. 1300.]\tOutput: 2899.84\tActual: 2900.0\n",
      "Input: [1608. 1304.]\tOutput: 2911.84\tActual: 2912.0\n",
      "Input: [1616. 1308.]\tOutput: 2923.84\tActual: 2924.0\n",
      "Input: [1624. 1312.]\tOutput: 2935.84\tActual: 2936.0\n",
      "Input: [1632. 1316.]\tOutput: 2947.83\tActual: 2948.0\n",
      "Input: [1640. 1320.]\tOutput: 2959.83\tActual: 2960.0\n",
      "Input: [1648. 1324.]\tOutput: 2971.83\tActual: 2972.0\n",
      "Input: [1656. 1328.]\tOutput: 2983.83\tActual: 2984.0\n",
      "Input: [1664. 1332.]\tOutput: 2995.83\tActual: 2996.0\n",
      "Input: [1672. 1336.]\tOutput: 3007.83\tActual: 3008.0\n",
      "Input: [1680. 1340.]\tOutput: 3019.83\tActual: 3020.0\n",
      "Input: [1688. 1344.]\tOutput: 3031.83\tActual: 3032.0\n",
      "Input: [1696. 1348.]\tOutput: 3043.83\tActual: 3044.0\n",
      "Input: [1704. 1352.]\tOutput: 3055.83\tActual: 3056.0\n",
      "Input: [1712. 1356.]\tOutput: 3067.83\tActual: 3068.0\n",
      "Input: [1720. 1360.]\tOutput: 3079.83\tActual: 3080.0\n",
      "Input: [1728. 1364.]\tOutput: 3091.83\tActual: 3092.0\n",
      "Input: [1736. 1368.]\tOutput: 3103.83\tActual: 3104.0\n",
      "Input: [1744. 1372.]\tOutput: 3115.83\tActual: 3116.0\n",
      "Input: [1752. 1376.]\tOutput: 3127.83\tActual: 3128.0\n",
      "Input: [1760. 1380.]\tOutput: 3139.82\tActual: 3140.0\n",
      "Input: [1768. 1384.]\tOutput: 3151.82\tActual: 3152.0\n",
      "Input: [1776. 1388.]\tOutput: 3163.82\tActual: 3164.0\n",
      "Input: [1784. 1392.]\tOutput: 3175.82\tActual: 3176.0\n",
      "Input: [1792. 1396.]\tOutput: 3187.82\tActual: 3188.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x_test))[:100]:\n",
    "    print(f\"Input: {x_test[i]}\\tOutput: {tf.matmul([x_test[i]], W)[0][0]:.2f}\\tActual: {y_test[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0. 5.]\tOutput: 5.00\tActual: 5.0\n",
      "Input: [ 5. 10.]\tOutput: 15.00\tActual: 15.0\n",
      "Input: [10. 15.]\tOutput: 25.00\tActual: 25.0\n",
      "Input: [15. 20.]\tOutput: 35.00\tActual: 35.0\n",
      "Input: [20. 25.]\tOutput: 45.00\tActual: 45.0\n",
      "Input: [25. 30.]\tOutput: 55.00\tActual: 55.0\n",
      "Input: [30. 35.]\tOutput: 65.00\tActual: 65.0\n",
      "Input: [35. 40.]\tOutput: 75.00\tActual: 75.0\n",
      "Input: [40. 45.]\tOutput: 84.99\tActual: 85.0\n",
      "Input: [45. 50.]\tOutput: 94.99\tActual: 95.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Input: {x_train[i]}\\tOutput: {tf.matmul([x_train[i]], W)[0][0]:.2f}\\tActual: {y_train[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
