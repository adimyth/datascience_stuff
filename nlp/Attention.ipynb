{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention is all you need\n",
    "\n",
    "Attention Mechanisms in Neural Networks are (very) loosely based on the visual attention mechanism found in humans\n",
    "\n",
    "**Issues with Seq2Seq Models**\n",
    "\n",
    "![seq-2-seq](https://lilianweng.github.io/lil-log/assets/images/encoder-decoder-example.png)\n",
    "\n",
    "* Unable to capture long term dependencies. Long-range dependencies still tricky despite of gating models like LSTMs and GRUs.\n",
    "* Handle input sequence word by word.\n",
    "* In translation systems, only the last hidden state is passed to the decoder, but it is not good enough to capture all the information pertaining to translation.\n",
    "\n",
    "> You can’t cram the meaning of a whole %&! # sentence into a single &!#* vector!\n",
    "\n",
    "In an attention mechanism, we do not encode the entire sequence into a single vector, rather allow decoder to attend to different parts of the source sentence at each step of the output generation. \n",
    "\n",
    "Each decoder output word depends on a weighted combination of all the input states, not just the last state. Attention allows us to interpret and visualise what the model is doing as heatmaps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECODER\n",
    "\n",
    "Rather than building a single context vector out of the encoder’s last hidden state, attention adds shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element. \n",
    "\n",
    "Consider, source sequence *x* of length *n* and target sequence *y* of length *m* \n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{x} &= [x_1, x_2, \\dots, x_n] \\\\\n",
    "\\mathbf{y} &= [y_1, y_2, \\dots, y_m]\n",
    "\\end{aligned}\n",
    "\n",
    "Encoder produces hiddent state $h_{i}$ for each input timestep. The decoder has hidden state, \n",
    "$s_{t}=f(s_{t-1}, y_{t-1}, c_{t})$ for $t=1, ..., m$, where\n",
    "\n",
    "\\begin{aligned}\n",
    "\\mathbf{c}_t &= \\sum_{i=1}^n \\alpha_{t,i} \\boldsymbol{h}_i & \\small{\\text{; Context vector for output }y_t}\\\\\n",
    "\\alpha_{t,i} &= \\text{align}(y_t, x_i) & \\small{\\text{; How well two words }y_t\\text{ and }x_i\\text{ are aligned.}}\\\\\n",
    "&= \\frac{\\exp(\\text{score}(\\boldsymbol{s}_{t-1}, \\boldsymbol{h}_i))}{\\sum_{i'=1}^n \\exp(\\text{score}(\\boldsymbol{s}_{t-1}, \\boldsymbol{h}_{i'}))} & \\small{\\text{; Softmax of some predefined alignment score.}}.\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "$\\alpha_{t,i}$ is the score for the input at position $i$ and output at position $t$ i.e for the pair $(y_{t}, x_{i})$\n",
    "\n",
    "\n",
    "**Bahdanau**\n",
    "\n",
    "$$score(s_t, h_i) = {v}_a^\\top \\tanh({W}_a[s_t;h_i])$$\n",
    "where both $v_a$ and $W_a$ are weight matrices to be learned in the alignment model.\n",
    "\n",
    "**Luong**\n",
    "\n",
    "$$score(s_t, h_i) = s_t^\\top\\mathbf{W}_a{h}_i$$\n",
    "where $W_a$ is a trainable weight matrix in the attention layer.\n",
    "\n",
    "Decoder RNN gives only the hidden state vector, which is used to calculate the *output vector*, as well as combined with the context vector to pass as input to the next decoder unit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CodeWise\n",
    "\n",
    ">score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n",
      "141\n",
      "550\n",
      "147\n",
      "43\n",
      "123\n",
      "562\n",
      "233\n",
      "130\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): print(len(X_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "for i in range(10): print(len(X_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
