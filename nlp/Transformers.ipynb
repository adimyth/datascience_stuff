{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "### Resources\n",
    "[Peter Bloem's Blog](http://www.peterbloem.nl/blog/transformers)\n",
    "\n",
    "[Jay Alammar's Blog](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "![transformer](https://miro.medium.com/max/1400/1*BHzGVskWGS_3jEcYYi6miQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "Self-attention is a sequence-to-sequence operation: a sequence of vectors goes in, and a sequence of vectors comes out. \n",
    "\n",
    "Let's call the input vectors $x_1, x_2, \\ldots x_t$ and the corresponding output vectors $y_1, y_2, \\ldots y_t$. The vectors all have dimension k. Output vector $y_i$ is simply a weighted average of all the input vectors.\n",
    "\n",
    "$$y_{i} = \\sum_{{j}} w_{{i}{j}} x_{j}$$\n",
    "\n",
    "$w_{{i}{j}}$ provides the relationsip between $x_{i}, x_{j}$\n",
    "\n",
    "$$w'_{{i}{j}} = {x_{i}}^Tx_{j}$$ The simplest thing that could be done is take a dot product between the two vectors as a measure of similarity.\n",
    "\n",
    "$$w_{{i}{j}} = \\frac{\\text{exp } w'_{{i}{j}}}{\\sum_{j} \\text{exp }w'_{{i}{j}}}$$\n",
    "\n",
    "![basic-self-attention](http://www.peterbloem.nl/files/transformers/self-attention.svg)\n",
    "\n",
    "In the example above to find the output vector $y_{2}$, we take a dot product between every vector $x_{i}$ and $x_{2}$, which gives us a weight vector $w_{2i}$. Multiply the weight vector $w_{2i}$ with corresponding input vector $x_{i}$ and sum them up to get the output vector $y_{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# x1, x2, ..., x10 with each vector of dimension 5\n",
    "x = np.random.random(size=(10, 5))\n",
    "weights = np.dot(x, x.T)\n",
    "\n",
    "# weights[i, j] represent weight of xj for xi\n",
    "for idx, w in enumerate(weights):\n",
    "    weights[idx] = np.exp(w)/sum(np.exp(w))\n",
    "\n",
    "# y1, y2, ..., y10 => output vector\n",
    "y = np.matmul(weights, x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query, Key and Value\n",
    "Every input vector $ùê±_{i}$ is used in three different ways in the self attention operation:\n",
    "\n",
    "* It is compared to every other vector to establish the weights for its own output $ùê≤_{i}$\n",
    "* It is compared to every other vector to establish the weights for the output of the j-th vector $ùê≤_{j}$\n",
    "* It is used as part of the weighted sum to compute each output vector once the weights have been established\n",
    "\n",
    "They are `query`, `key` and `value` respectively. We add three **$k√ók$** weight matrices $ùêñ_{q}$, $ùêñ_{k}$, $ùêñ_{v}$ and compute three linear transformations of each $x_{i}$, for the three different parts of the self attention\n",
    "![img](https://i.imgur.com/OhTQX01.png)\n",
    "\n",
    "**Shapes**\n",
    "* $W_{q}, W_{k}, W_{v} - kxk$ matrix\n",
    "* $q_{i}, k_{i}, v_{i} - k$ dimensional vector\n",
    "* $w'_{{i}{j}}$ - scalar value\n",
    "* $w_{{i}{j}}$ - scalar value\n",
    "* $y_{i} - k$ dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Dot Product\n",
    "\n",
    "As the number of dimensions **$k$** increases, the dot-product $w'_{{i}{j}}$ grows too. Softmax is sensitive to large values, as the value increases the curve tends to flatten & hence resulting in smaller gradients. This in turn slows down learning. Hence, we scale down the value $w'_{{i}{j}}$ by dividing it by $\\sqrt{k}$. \n",
    "\n",
    "Why $\\sqrt{k}$? Imagine a vector in $‚Ñù^{k}$ with values all c. Its Euclidean length is $k\\sqrt{c}$.\n",
    "\n",
    "```python\n",
    "len1 = 256\n",
    "x = np.array([10]*len1)\n",
    "z1 = np.dot(x, x.T)\n",
    "print(z1, z1/np.sqrt(len1)) # 25600 1600.0\n",
    "\n",
    "len2 = 512\n",
    "y = np.array([10]*len2)\n",
    "z2 = np.dot(y, y.T)\n",
    "print(z2, z2/np.sqrt(l2)) # 51200 2262.74\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "```python\n",
    "def SingleHeadAttention(query, key, value):\n",
    "    \"\"\"\n",
    "    query - (batch_size, query_len, embedding_size)\n",
    "    key - (batch_size, value_len, embedding_size)\n",
    "    value - (batch_size, value_len, embedding_size)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1 - Matrix multiplication between query and key\n",
    "    # weights --> (batch_size, query_len, value_len)\n",
    "    weights = tf.matmul(query, value)\n",
    "\n",
    "    # Step 2 - Weights Normalization\n",
    "    # weights --> (batch_size, query_len, value_len)\n",
    "    dim = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    weights = weights/tf.math.sqrt(dim)\n",
    "    \n",
    "    # Step 3 - Softmax scores\n",
    "    # weights --> (batch_size, query_len, value_len)\n",
    "    weights = tf.nn.softmax(weights, axis=-1)\n",
    "    \n",
    "    # Step 4 - Context Vector\n",
    "    context = tf.matmul(weights, value)\n",
    "    \n",
    "    return context\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHead Attention\n",
    "\n",
    "Each word may have different meaning depending on the context. Mutlihead attention gives the attention layer multiple ‚Äúrepresentation subspaces‚Äù.\n",
    "\n",
    "![mutlihead](https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Embeddings represent a token in a $k-dimensional$ space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence.\n",
    "\n",
    "Since we have **no recurrent networks that can remember how sequences are fed into a model, we need to somehow give every word/part in our sequence a relative position** since a sequence depends on the order of its elements. These positions are added to the embedded representation of each word.\n",
    "\n",
    "So after adding the positional encoding, **words will be closer to each other based on the similarity of their meaning and their position in the sentence**, in the $k-dimensional$ space.\n",
    "\n",
    "![positional-encoding](https://i.imgur.com/Kc78rpW.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
